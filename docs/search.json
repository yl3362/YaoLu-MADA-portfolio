[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "#Background\nMy name is Yao Lu. I am a second year PhD student concentrating in data analysis and modeling. I completed my undergraduate education at Qingdao University, majoy in math and minor in finance. I completed master education at University of Dayton with math major focusing on statistics.\n#Goals for MADA\nI have learned some theoretical statistical inference. The most parts I’d like to learn is how to transform the theoretical knowledge into application about solving the problem in real world. And how to present to others in a simple and clear way.\n#Research interests\nMy primary interest is in infectious disease, especially influenza. Recently I am exploring the ferret vaccination data.\n#fun fact\nI lived in Qingdao(China) for more than twenty years, which is a beautiful costal city. There are many delicious seafood there. Qingdao is a city which is surrounded by mountains and sea. Before I moved to Gerogia, I lived two years in Ohio which has a colder winter, and Ohio has thick snow there in winter.\n#Experience\nBefore I came to UGA, I had math/statistics backgrounds. I have used R/SAS for solving and displaying statistics problems. I have used Matlab for solving PDF problems. I have used Latex for display statistics problems and formulas."
  },
  {
    "objectID": "coding_exercise.html",
    "href": "coding_exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise.\n\n#load dslabs package\nlibrary(\"dslabs\")\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n#look at help file for gapminder data\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n#select only africa\nafricadata <- gapminder[which(gapminder$continent=='Africa'),]\n#get an overview of data structure\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n#get a summary of data\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n#infant_mortality and life_expectancy\nmoex <- africadata %>% select(c('infant_mortality','life_expectancy'))\n#population and life_expectancy\npoex <- africadata %>% select(c('population','life_expectancy'))\n#get an overview of data structure\nstr(moex)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nstr(poex)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n#get a summary of data\nsummary(moex)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nsummary(poex)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#scatter plot infant mortality VS life expectancy\nplot(moex$infant_mortality,moex$life_expectancy, main=\"infant mortality VS life expectancy\", xlab=\"infant mortality\", ylab=\"life expectancy\", pch=19)\n\n\n\n\n\n#scatter plot population size VS life expectancy\nplot(poex$population,poex$life_expectancy, main=\"population size VS life expectancy\", xlab=\"population size\", ylab=\"life expectancy\", pch=19,log='x')\n\n\n\n\n\n#the year which infant_mortality is NA\nnamor <-africadata$year[which(is.na(africadata$infant_mortality))]\ntable(namor)\n\nnamor\n1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 \n  10   17   16   16   15   14   13   11   11    7    5    6    6    6    5    5 \n1976 1977 1978 1979 1980 1981 2016 \n   3    3    2    2    1    1   51 \n\n\n\n#2000 year\nafri2000 <- africadata[which(africadata$year==2000),]\nstr(afri2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(afri2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n#plot \nplot(afri2000$infant_mortality,afri2000$life_expectancy, main=\"infant mortality VS life expectancy\", xlab=\"infant mortality\", ylab=\"life expectancy\", pch=19)\n\n\n\n#plot\nplot(afri2000$population,afri2000$life_expectancy, main=\"population size VS life expectancy\", xlab=\"population size\", ylab=\"life expectancy\", pch=19,log='x')\n\n\n\n\n\nfit1 <- lm(life_expectancy~infant_mortality,data=afri2000)\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = afri2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      71.29331    2.42611  29.386  < 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nfit2 <- lm(life_expectancy~population,data=afri2000)\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = afri2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   <2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nHere we know infant mortality has statistically significant negative relationship with life-expectancy. But we don’t have enough statistical evidence about the relationship between population and life expectancy.\n————————————————————————————————————————–\nTHIS SECTION ADDED BY SHIWANI SAPKOTA\nI am interested on looking at infant mortality, life expectancy, fertility, and population variables for European countries in the year 2010 using gapminder data.\nDATA PROCESSING\n\n# Creating an object europedata containing only the European countries\neuropedata <- gapminder %>% filter(continent == \"Europe\")\n\n# Looking at the structure and summary of object (europedata)\nstr(europedata)\n\n'data.frame':   2223 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 9 15 16 21 25 42 45 46 54 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 37.3 NA 29.5 105 ...\n $ life_expectancy : num  62.9 68.8 71.6 69.6 62.6 ...\n $ fertility       : num  6.19 2.7 2.74 2.6 4.05 2.25 2.33 2.3 2.54 1.95 ...\n $ population      : num  1636054 7065525 8190027 9140563 3214520 ...\n $ gdp             : num  NA 5.24e+10 NA 6.82e+10 NA ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 22 7 22 19 7 19 7 13 13 ...\n\nsummary(europedata)\n\n                   country          year      infant_mortality life_expectancy\n Albania               :  57   Min.   :1960   Min.   :  1.50   Min.   :60.85  \n Austria               :  57   1st Qu.:1974   1st Qu.:  5.80   1st Qu.:70.49  \n Belarus               :  57   Median :1988   Median : 11.25   Median :73.11  \n Belgium               :  57   Mean   :1988   Mean   : 15.33   Mean   :73.56  \n Bosnia and Herzegovina:  57   3rd Qu.:2002   3rd Qu.: 19.50   3rd Qu.:76.80  \n Bulgaria              :  57   Max.   :2016   Max.   :120.00   Max.   :83.30  \n (Other)               :1881                  NA's   :305                     \n   fertility       population             gdp               continent   \n Min.   :1.130   Min.   :   175520   Min.   :6.277e+08   Africa  :   0  \n 1st Qu.:1.510   1st Qu.:  3261643   1st Qu.:1.278e+10   Americas:   0  \n Median :1.870   Median :  7614832   Median :7.388e+10   Asia    :   0  \n Mean   :1.968   Mean   : 17891944   Mean   :2.295e+11   Europe  :2223  \n 3rd Qu.:2.232   3rd Qu.: 13400360   3rd Qu.:2.167e+11   Oceania :   0  \n Max.   :6.190   Max.   :148435811   Max.   :2.131e+12                  \n NA's   :39      NA's   :39          NA's   :789                        \n                       region   \n Southern Europe          :684  \n Eastern Europe           :570  \n Northern Europe          :570  \n Western Europe           :399  \n Australia and New Zealand:  0  \n Caribbean                :  0  \n (Other)                  :  0  \n\nglimpse(europedata)\n\nRows: 2,223\nColumns: 9\n$ country          <fct> \"Albania\", \"Austria\", \"Belarus\", \"Belgium\", \"Bosnia a…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.4, 37.3, NA, 29.5, 105.0, 49.0, 70.0, 22.0, 21.3,…\n$ life_expectancy  <dbl> 62.87, 68.75, 71.59, 69.59, 62.56, 69.22, 64.85, 70.5…\n$ fertility        <dbl> 6.19, 2.70, 2.74, 2.60, 4.05, 2.25, 2.33, 2.30, 2.54,…\n$ population       <dbl> 1636054, 7065525, 8190027, 9140563, 3214520, 7866472,…\n$ gdp              <dbl> NA, 52392699681, NA, 68236665814, NA, NA, NA, NA, 521…\n$ continent        <fct> Europe, Europe, Europe, Europe, Europe, Europe, Europ…\n$ region           <fct> Southern Europe, Western Europe, Eastern Europe, West…\n\n\n\n# Looking at the years having missing data for infant mortality in europedata\ninfantmort_yrs_europe <- europedata %>% \n                        filter(is.na(infant_mortality))\ntable(infantmort_yrs_europe$year)\n\n\n1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 \n   6   19   19   18   17   17   17   16   14   14    3   11   11   11   11   11 \n1976 1977 1978 1979 1980 1981 1982 1983 2016 \n  11   11   11   10    2    2    2    2   39 \n\n\n\n# Creating a new object named europedata_2010 by using only year 2010 data from europedata\neuropedata_2010 <- europedata %>%\n                   filter(year == 2010)\n\n# Looking at the structure and summary of object (europedata_2010)\nstr(europedata_2010)\n\n'data.frame':   39 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 9 15 16 21 25 42 45 46 54 ...\n $ year            : int  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...\n $ infant_mortality: num  14.8 3.6 4.7 3.6 6.4 11.2 4.6 3.4 3.3 3.6 ...\n $ life_expectancy : num  77.2 80.5 70.2 80.1 77.9 73.7 76.7 77.5 79.4 76.4 ...\n $ fertility       : num  1.74 1.44 1.46 1.84 1.24 1.49 1.47 1.5 1.88 1.63 ...\n $ population      : num  2901883 8391986 9492122 10929978 3835258 ...\n $ gdp             : num  6.14e+09 2.24e+11 2.60e+10 2.67e+11 8.21e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 22 7 22 19 7 19 7 13 13 ...\n\nsummary(europedata_2010)\n\n                   country        year      infant_mortality life_expectancy\n Albania               : 1   Min.   :2010   Min.   : 1.900   Min.   :68.90  \n Austria               : 1   1st Qu.:2010   1st Qu.: 3.450   1st Qu.:75.00  \n Belarus               : 1   Median :2010   Median : 4.100   Median :79.40  \n Belgium               : 1   Mean   :2010   Mean   : 5.544   Mean   :77.73  \n Bosnia and Herzegovina: 1   3rd Qu.:2010   3rd Qu.: 6.500   3rd Qu.:80.90  \n Bulgaria              : 1   Max.   :2010   Max.   :14.800   Max.   :82.80  \n (Other)               :33                                                  \n   fertility       population             gdp               continent \n Min.   :1.240   Min.   :   318042   Min.   :1.405e+09   Africa  : 0  \n 1st Qu.:1.440   1st Qu.:  3479046   1st Qu.:1.437e+10   Americas: 0  \n Median :1.490   Median :  7830534   Median :5.653e+10   Asia    : 0  \n Mean   :1.587   Mean   : 18843145   Mean   :2.759e+11   Europe  :39  \n 3rd Qu.:1.770   3rd Qu.: 13904540   3rd Qu.:2.591e+11   Oceania : 0  \n Max.   :2.120   Max.   :143158099   Max.   :2.069e+12                \n                                                                      \n                       region  \n Southern Europe          :12  \n Eastern Europe           :10  \n Northern Europe          :10  \n Western Europe           : 7  \n Australia and New Zealand: 0  \n Caribbean                : 0  \n (Other)                  : 0  \n\nglimpse(europedata_2010)\n\nRows: 39\nColumns: 9\n$ country          <fct> \"Albania\", \"Austria\", \"Belarus\", \"Belgium\", \"Bosnia a…\n$ year             <int> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,…\n$ infant_mortality <dbl> 14.8, 3.6, 4.7, 3.6, 6.4, 11.2, 4.6, 3.4, 3.3, 3.6, 2…\n$ life_expectancy  <dbl> 77.2, 80.5, 70.2, 80.1, 77.9, 73.7, 76.7, 77.5, 79.4,…\n$ fertility        <dbl> 1.74, 1.44, 1.46, 1.84, 1.24, 1.49, 1.47, 1.50, 1.88,…\n$ population       <dbl> 2901883, 8391986, 9492122, 10929978, 3835258, 7407297…\n$ gdp              <dbl> 6.137564e+09, 2.239541e+11, 2.600213e+10, 2.671431e+1…\n$ continent        <fct> Europe, Europe, Europe, Europe, Europe, Europe, Europ…\n$ region           <fct> Southern Europe, Western Europe, Eastern Europe, West…\n\n\n\n# Looking at the top 5 European countries having highest infant mortality for the year 2010\neurope_infantmort <- europedata_2010 %>% select (country, infant_mortality) %>%\narrange(desc(`infant_mortality`))\neurope2010_infantmort <- head(europe_infantmort, 5)\neurope2010_infantmort\n\n   country infant_mortality\n1  Albania             14.8\n2  Moldova             14.7\n3  Romania             12.1\n4 Bulgaria             11.2\n5   Russia             10.3\n\n# Looking at the top 5 European countries having highest life expectancy for the year 2010\neurope_lifeexpect <- europedata_2010 %>% select (country, life_expectancy) %>%\narrange(desc(`life_expectancy`))\neurope2010_lifeexpect <- head(europe_lifeexpect, 5)\neurope2010_lifeexpect\n\n      country life_expectancy\n1     Iceland            82.8\n2 Switzerland            82.3\n3       Italy            81.9\n4       Spain            81.8\n5      Sweden            81.6\n\n# Looking at the top 5 European countries having largest population size for the year 2010\neurope_pop <- europedata_2010 %>% select (country, population) %>%\narrange(desc(`population`))\neurope2010_pop <- head(europe_pop, 5)\neurope2010_pop\n\n         country population\n1         Russia  143158099\n2        Germany   80435307\n3         France   62961136\n4 United Kingdom   62716684\n5          Italy   59588007\n\n\nDATA VISUALIZATION\n\n# Creating the object country to sort the 5 European countries having the highest infant mortality in the year 2010\neurope2010_infantmort <- europe2010_infantmort %>%\nmutate(country = as_factor(country),\ncountry <- fct_reorder(country, infant_mortality))\n\n# Plotting the horizontal barplot for visualizing top 5 European countries with the highest infant mortality in 2010\nggplot(data = europe2010_infantmort, aes(x = country, y = `infant_mortality`)) +\ngeom_bar(stat = \"identity\", width = 0.75, color = \"blue\", fill = \"steelblue\") +\ncoord_flip() +\nlabs(title = \"Top 5 European countries with the highest infant mortality in 2010\",\nx = \"Country\", y = \"Infant Mortality\") +\ntheme_classic() +\ntheme(axis.text = element_text(face=\"bold\"), plot.title = element_text(hjust = 0.5, size = 14),\naxis.title = element_text(size = 14))\n\n\n\n\n\n# Plotting fertility as a function of infant mortality in the year 2010 for European countries\nggplot(data = europedata_2010,\n       (aes(x = infant_mortality, y = fertility))) +\n  theme_classic() +\n  geom_point(size = 1.0) +\n  labs(title = \"Fertility as a function of infant mortality in the year 2010 for European countries\",\n       x = \"Infant Mortality\", y = \"Fertility\") +\n  theme(axis.text = element_text(face = \"bold\"), plot.title = element_text(hjust = 0.5, size = 14),\n        axis.title = element_text(size = 14))\n\n\n\n\n\n# Plotting life expectancy as a function of population size in the year 2010 for European countries\nggplot(data = europedata_2010,\n       (aes(x = population, y = life_expectancy))) +\n  theme_classic() +\n  geom_point(size = 1.0) +\n  scale_x_continuous(trans = 'log10') +\n  labs(title = \"Life expectancy as a function of population size in the year 2010 for European countries\", \n       x = \"Population (In Logscale)\", y = \"Life Expectancy (In Years)\") +\n  theme(axis.text = element_text(face = \"bold\"), plot.title =   element_text(hjust = 0.5, size = 14),\n        axis.title = element_text(size = 14))\n\n\n\n\nFITTING MODEL\n\n# Using lm function to fit linear regression model using fertility as the outcome and infant mortality as the predictor for the year 2010\nfit3 <- lm(fertility ~ infant_mortality, data = europedata_2010)\n# Tabulating the output fit3 from lm using broom package\nbroom::tidy(fit3)\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        1.71      0.0691     24.8  1.25e-24\n2 infant_mortality  -0.0224    0.0107     -2.10 4.29e- 2\n\n# Using lm function to fit linear regression model using life expectancy as the outcome and population size as the predictor for the year 2010\nfit4 <- lm(life_expectancy ~ population, data = europedata_2010)\n# Tabulating the output fit4 from lm using broom package\nbroom::tidy(fit4)\n\n# A tibble: 2 × 5\n  term             estimate    std.error statistic  p.value\n  <chr>               <dbl>        <dbl>     <dbl>    <dbl>\n1 (Intercept) 78.1          0.730          107.    1.05e-47\n2 population  -0.0000000203 0.0000000213    -0.956 3.45e- 1\n\n\nCONCLUSION\nBased on the p-values for each fit, we found somewhat statistically significant negative correlation between infant mortality and fertility in the year 2010 for European countries (p-value: 0.0429) while we found no statistically significant correlation between population size and life expectancy in the year 2010 for European countries (p-value: 0.345 )."
  },
  {
    "objectID": "dataanalysis_exercise.html",
    "href": "dataanalysis_exercise.html",
    "title": "dataanalysis_exercise",
    "section": "",
    "text": "The data was found at https://data.cdc.gov/NCHS/Weekly-Counts-of-Deaths-by-Jurisdiction-and-Age/y5bj-9g5w\nThis data include 11 variables. This data set describes a time series data for each state by age.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\ndata1 <- read.csv(\"mod4/rawdata/Weekly_Counts_of_Deaths_by_Jurisdiction_and_Age.csv\")\n\np <- ggplot(data = data1[which(data1$Jurisdiction==\"Georgia\"&data1$Type=='Predicted (weighted)'),], aes(x = Week, y = Number.of.Deaths, group = Age.Group,color=Age.Group))\np + geom_line()+ggtitle(\"number of death(weighted) in GA\") +  xlab(\"week\") + ylab(\"death\")\n\n\n\n\nHere we can see that most death happened at aged people. Few young people die. Then we check the ‘Week’ variables. We can see that The week is the number of week in each year. Then we want to see a time series week.\n\n#redefine the week variable\ntable(data1$Year[which(data1$Week==53)])\n\n\n2020 \n 592 \n\ntable(data1$Year[which(data1$Week==52)])\n\n\n2015 2016 2017 2018 2019 2020 2021 2022 \n 582  598  590  584  588  596  608  561 \n\ndata1$newweek <- NA\ndata1$newweek[which(data1$Year==2015)] <- data1$Week[which(data1$Year==2015)]\ndata1$newweek[which(data1$Year==2016)] <- data1$Week[which(data1$Year==2016)]+52\ndata1$newweek[which(data1$Year==2017)] <- data1$Week[which(data1$Year==2017)]+52*2\ndata1$newweek[which(data1$Year==2018)] <- data1$Week[which(data1$Year==2018)]+52*3\ndata1$newweek[which(data1$Year==2019)] <- data1$Week[which(data1$Year==2019)]+52*4\ndata1$newweek[which(data1$Year==2020)] <- data1$Week[which(data1$Year==2020)]+52*5\ndata1$newweek[which(data1$Year==2021)] <- data1$Week[which(data1$Year==2021)]+52*6+1\ndata1$newweek[which(data1$Year==2022)] <- data1$Week[which(data1$Year==2022)]+52*7+1\n\n\np <- ggplot(data = data1[which(data1$Jurisdiction==\"Georgia\"&data1$Type=='Predicted (weighted)'),], aes(x = newweek, y = Number.of.Deaths, group = Age.Group,color=Age.Group))\np + geom_line()+ggtitle(\"number of death(weighted) in GA\") +  xlab(\"week\") + ylab(\"death\")\n\nWarning: Removed 16 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWe can see that there is a peak at around 350 weeks. Here we can see that most death happened at aged people. Few young people die.\nWe can apply time series analysis. Such as using ACF, PACF to find best ARIMA model.\n\nsaveRDS(data1, file = \"mod4/processdata/cleandata.rds\")\n\n\ntable(data1$Age.Group)\n\n\n       25-44 years        45-64 years        65-74 years        75-84 years \n             37940              45293              45287              45304 \n85 years and older     Under 25 years \n             45280              29494 \n\ndata1$Age.Group[which(data1$Age.Group=='Under 25 years')] <- '24 or younger'\n\n\nboxplot(Number.of.Deaths~Age.Group,data=data1[which(data1$Jurisdiction==\"Georgia\"&data1$Type=='Predicted (weighted)'),], main=\"number of deaths in GA\",        xlab=\"age\", ylab=\"deaths\")\n\n\n\n\n\n—————————————\nTHIS SECTION ADDED BY IRENE CAVROS\n—————————————\n\nLoad packages\n\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\n\n\nRead and check clean RDS file\n\ndeathdata_clean_INC <- readRDS(\"mod4/processdata/cleandata.rds\")\n\nstr(deathdata_clean_INC)\n\n'data.frame':   248598 obs. of  12 variables:\n $ Jurisdiction      : chr  \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ Week.Ending.Date  : chr  \"01/10/2015\" \"01/17/2015\" \"01/24/2015\" \"01/31/2015\" ...\n $ State.Abbreviation: chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ Year              : int  2015 2015 2015 2015 2015 2015 2015 2015 2015 2015 ...\n $ Week              : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Age.Group         : chr  \"25-44 years\" \"25-44 years\" \"25-44 years\" \"25-44 years\" ...\n $ Number.of.Deaths  : int  67 49 55 59 47 59 41 47 60 57 ...\n $ Time.Period       : chr  \"2015-2019\" \"2015-2019\" \"2015-2019\" \"2015-2019\" ...\n $ Type              : chr  \"Predicted (weighted)\" \"Predicted (weighted)\" \"Predicted (weighted)\" \"Predicted (weighted)\" ...\n $ Suppress          : chr  \"\" \"\" \"\" \"\" ...\n $ Note              : chr  \"\" \"\" \"\" \"\" ...\n $ newweek           : num  1 2 3 4 5 6 7 8 9 10 ...\n\nhead(deathdata_clean_INC)\n\n  Jurisdiction Week.Ending.Date State.Abbreviation Year Week   Age.Group\n1      Alabama       01/10/2015                 AL 2015    1 25-44 years\n2      Alabama       01/17/2015                 AL 2015    2 25-44 years\n3      Alabama       01/24/2015                 AL 2015    3 25-44 years\n4      Alabama       01/31/2015                 AL 2015    4 25-44 years\n5      Alabama       02/07/2015                 AL 2015    5 25-44 years\n6      Alabama       02/14/2015                 AL 2015    6 25-44 years\n  Number.of.Deaths Time.Period                 Type Suppress Note newweek\n1               67   2015-2019 Predicted (weighted)                     1\n2               49   2015-2019 Predicted (weighted)                     2\n3               55   2015-2019 Predicted (weighted)                     3\n4               59   2015-2019 Predicted (weighted)                     4\n5               47   2015-2019 Predicted (weighted)                     5\n6               59   2015-2019 Predicted (weighted)                     6\n\n\n\nIt looks like there are observations here with state abbreviation ‘US.’ I am going to remove these since it is likely an aggregate of all states and not an actual state in itself. But before I do this, I will look at deaths in the US by year.\n\n\n\nLook at US only data\n\nUS <- deathdata_clean_INC %>%\n  filter(State.Abbreviation %in% \"US\")\n\n\n\nPlot of US data by year\n\nggplot (US, aes(x=Year, y=Number.of.Deaths)) + \n  geom_bar(stat = 'identity') + \n  labs(x = \"Year\", y = \"Deaths\", \n       title = \"Deaths in the US by year\")\n\n\n\n\n\n\nNew smaller dataset only looking at only 4 variables,\n\ndeaths_subset_INC <- deathdata_clean_INC %>%\n    select(State.Abbreviation, Year, Age.Group, Number.of.Deaths)\n\n\n\nRemove observations with state abbreviation US\n\nstate_deaths_INC <- deaths_subset_INC %>%\n    filter(!State.Abbreviation %in% c('US'))\n\n\n\nSubset for year 2015\n\ndeaths_2015_INC <- state_deaths_INC %>% \n    filter(Year==2015)\n\n\n\nSubset for year 2016\n\ndeaths_2016_INC <- state_deaths_INC %>% \n    filter(Year==2016)\n\n\n\nSubset for year 2017\n\ndeaths_2017_INC <- state_deaths_INC %>% \n    filter(Year==2017)\n\n\n\nSubset for year 2018\n\ndeaths_2018_INC <- state_deaths_INC %>% \n    filter(Year==2018)\n\n\n\nSubset for year 2019\n\ndeaths_2019_INC <- state_deaths_INC %>% \n    filter(Year==2019)\n\n\n\nBar graph of deaths by state in the year 2015\n\nggplot (state_deaths_INC, aes(x=State.Abbreviation, y=Number.of.Deaths)) + \n  geom_bar(stat = 'identity') + \n  coord_flip() +\n  labs(x = \"State\", y = \"Deaths\", \n       title = \"Deaths by State in 2015\")\n\nWarning: Removed 32 rows containing missing values (`position_stack()`)."
  },
  {
    "objectID": "fluanalysis/code/exploration.html",
    "href": "fluanalysis/code/exploration.html",
    "title": "exploration",
    "section": "",
    "text": "library(DataExplorer)\nlibrary(skimr)\nlibrary(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\nlibrary(ggplot2)\n#load data\nexp1 <- readRDS(here(\"fluanalysis\", \"data\", \"processed_data.rds\"))\n\nskim(exp1)\n\n\nData summary\n\n\nName\nexp1\n\n\nNumber of rows\n730\n\n\nNumber of columns\n28\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n27\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSwollenLymphNodes\n0\n1\nFALSE\n2\nNo: 418, Yes: 312\n\n\nChestCongestion\n0\n1\nFALSE\n2\nYes: 407, No: 323\n\n\nChillsSweats\n0\n1\nFALSE\n2\nYes: 600, No: 130\n\n\nNasalCongestion\n0\n1\nFALSE\n2\nYes: 563, No: 167\n\n\nSneeze\n0\n1\nFALSE\n2\nYes: 391, No: 339\n\n\nFatigue\n0\n1\nFALSE\n2\nYes: 666, No: 64\n\n\nSubjectiveFever\n0\n1\nFALSE\n2\nYes: 500, No: 230\n\n\nHeadache\n0\n1\nFALSE\n2\nYes: 615, No: 115\n\n\nWeakness\n0\n1\nFALSE\n4\nMod: 338, Mil: 223, Sev: 120, Non: 49\n\n\nCoughIntensity\n0\n1\nFALSE\n4\nMod: 357, Sev: 172, Mil: 154, Non: 47\n\n\nMyalgia\n0\n1\nFALSE\n4\nMod: 325, Mil: 213, Sev: 113, Non: 79\n\n\nRunnyNose\n0\n1\nFALSE\n2\nYes: 519, No: 211\n\n\nAbPain\n0\n1\nFALSE\n2\nNo: 639, Yes: 91\n\n\nChestPain\n0\n1\nFALSE\n2\nNo: 497, Yes: 233\n\n\nDiarrhea\n0\n1\nFALSE\n2\nNo: 631, Yes: 99\n\n\nEyePn\n0\n1\nFALSE\n2\nNo: 617, Yes: 113\n\n\nInsomnia\n0\n1\nFALSE\n2\nYes: 415, No: 315\n\n\nItchyEye\n0\n1\nFALSE\n2\nNo: 551, Yes: 179\n\n\nNausea\n0\n1\nFALSE\n2\nNo: 475, Yes: 255\n\n\nEarPn\n0\n1\nFALSE\n2\nNo: 568, Yes: 162\n\n\nHearing\n0\n1\nFALSE\n2\nNo: 700, Yes: 30\n\n\nPharyngitis\n0\n1\nFALSE\n2\nYes: 611, No: 119\n\n\nBreathless\n0\n1\nFALSE\n2\nNo: 436, Yes: 294\n\n\nToothPn\n0\n1\nFALSE\n2\nNo: 565, Yes: 165\n\n\nVision\n0\n1\nFALSE\n2\nNo: 711, Yes: 19\n\n\nVomit\n0\n1\nFALSE\n2\nNo: 652, Yes: 78\n\n\nWheeze\n0\n1\nFALSE\n2\nNo: 510, Yes: 220\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nBodyTemp\n0\n1\n98.94\n1.2\n97.2\n98.2\n98.5\n99.3\n103.1\n▇▇▂▁▁\n\n\n\n\n#Summary\nsummary(exp1)\n\n SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion Sneeze   \n No :418           No :323         No :130      No :167         No :339  \n Yes:312           Yes:407         Yes:600      Yes:563         Yes:391  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Fatigue   SubjectiveFever Headache      Weakness    CoughIntensity\n No : 64   No :230         No :115   None    : 49   None    : 47   \n Yes:666   Yes:500         Yes:615   Mild    :223   Mild    :154   \n                                     Moderate:338   Moderate:357   \n                                     Severe  :120   Severe  :172   \n                                                                   \n                                                                   \n     Myalgia    RunnyNose AbPain    ChestPain Diarrhea  EyePn     Insomnia \n None    : 79   No :211   No :639   No :497   No :631   No :617   No :315  \n Mild    :213   Yes:519   Yes: 91   Yes:233   Yes: 99   Yes:113   Yes:415  \n Moderate:325                                                              \n Severe  :113                                                              \n                                                                           \n                                                                           \n ItchyEye  Nausea    EarPn     Hearing   Pharyngitis Breathless ToothPn  \n No :551   No :475   No :568   No :700   No :119     No :436    No :565  \n Yes:179   Yes:255   Yes:162   Yes: 30   Yes:611     Yes:294    Yes:165  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Vision    Vomit     Wheeze       BodyTemp     \n No :711   No :652   No :510   Min.   : 97.20  \n Yes: 19   Yes: 78   Yes:220   1st Qu.: 98.20  \n                               Median : 98.50  \n                               Mean   : 98.94  \n                               3rd Qu.: 99.30  \n                               Max.   :103.10  \n\n#Histogram and Density Plot\nplot_histogram(exp1)\n\n\n\nplot_density(exp1)\n\n\n\n#boxplot plot for SwollenLymphNodes\nggplot(exp1,aes(x=SwollenLymphNodes, y=BodyTemp))+\n  geom_boxplot()\n\n\n\n#boxplot plot for ChestCongestion\nggplot(exp1,aes(x=ChestCongestion, y=BodyTemp))+\n  geom_boxplot()"
  },
  {
    "objectID": "fluanalysis/code/fitting.html",
    "href": "fluanalysis/code/fitting.html",
    "title": "fitting",
    "section": "",
    "text": "library(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\nlibrary(performance)\n\nfit1 <- readRDS(here(\"fluanalysis\", \"data\", \"processed_data.rds\"))\n\n#modeling with RunnyNose\n\nlm1 <- lm(BodyTemp~RunnyNose,data=fit1)\n\n#modeling with all\n\nlm2 <- lm(BodyTemp~.,data=fit1)\n\n#compare lm1 and lm2\nAIC(lm1)\n\n[1] 2329.346\n\nAIC(lm2)\n\n[1] 2303.623\n\nanova(lm1,lm2)\n\nAnalysis of Variance Table\n\nModel 1: BodyTemp ~ RunnyNose\nModel 2: BodyTemp ~ SwollenLymphNodes + ChestCongestion + ChillsSweats + \n    NasalCongestion + Sneeze + Fatigue + SubjectiveFever + Headache + \n    Weakness + CoughIntensity + Myalgia + RunnyNose + AbPain + \n    ChestPain + Diarrhea + EyePn + Insomnia + ItchyEye + Nausea + \n    EarPn + Hearing + Pharyngitis + Breathless + ToothPn + Vision + \n    Vomit + Wheeze\n  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n1    728 1030.53                                  \n2    696  911.35 32    119.19 2.8445 4.933e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth AIC method and anove test show that lm2 is better than lm1 statistically.\n\n#logit model with RunnyNose\n\nlogit1 <- glm(Nausea ~RunnyNose, data = fit1, family = \"binomial\")\n\n#logit model will all other\n\nlogit2 <- glm(Nausea ~., data = fit1, family = \"binomial\")\n\nlogit1\n\n\nCall:  glm(formula = Nausea ~ RunnyNose, family = \"binomial\", data = fit1)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n    -0.65781       0.05018  \n\nDegrees of Freedom: 729 Total (i.e. Null);  728 Residual\nNull Deviance:      944.7 \nResidual Deviance: 944.6    AIC: 948.6\n\nlogit2\n\n\nCall:  glm(formula = Nausea ~ ., family = \"binomial\", data = fit1)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n              0.354800               -0.249429                0.267908  \n       ChillsSweatsYes      NasalCongestionYes               SneezeYes  \n              0.278658                0.427414                0.177783  \n            FatigueYes      SubjectiveFeverYes             HeadacheYes  \n              0.229345                0.273061                0.333450  \n          WeaknessMild        WeaknessModerate          WeaknessSevere  \n             -0.118025                0.314540                0.831106  \n    CoughIntensityMild  CoughIntensityModerate    CoughIntensitySevere  \n             -0.336382               -0.497253               -1.083273  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n             -0.003279                0.207185                0.123741  \n          RunnyNoseYes               AbPainYes            ChestPainYes  \n              0.039946                0.938752                0.069675  \n           DiarrheaYes                EyePnYes             InsomniaYes  \n              1.062315               -0.336984                0.083388  \n           ItchyEyeYes                EarPnYes              HearingYes  \n             -0.064552               -0.184794                0.327093  \n        PharyngitisYes           BreathlessYes              ToothPnYes  \n              0.279516                0.527816                0.483253  \n             VisionYes                VomitYes               WheezeYes  \n              0.120227                2.458305               -0.307664  \n              BodyTemp  \n             -0.032682  \n\nDegrees of Freedom: 729 Total (i.e. Null);  696 Residual\nNull Deviance:      944.7 \nResidual Deviance: 751.5    AIC: 819.5\n\n\nAIC shows that logit2 is better than logit1."
  },
  {
    "objectID": "fluanalysis/code/fitting.html#fitting-by-the-method-in-this-module",
    "href": "fluanalysis/code/fitting.html#fitting-by-the-method-in-this-module",
    "title": "fitting",
    "section": "fitting by the method in this module",
    "text": "fitting by the method in this module\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n\n✔ broom        1.0.4     ✔ recipes      1.0.5\n✔ dials        1.1.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.1     ✔ tibble       3.2.1\n✔ ggplot2      3.4.1     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::mae()  masks performance::mae()\n✖ yardstick::rmse() masks performance::rmse()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\n#set the engine\nengine1<- linear_reg() %>%\n  set_engine(\"lm\") \n\n#fit the model by RunnyNose\n\nlm1a <- engine1 %>%  fit(BodyTemp~RunnyNose, data=fit1)\nlm1a\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ RunnyNose, data = data)\n\nCoefficients:\n (Intercept)  RunnyNoseYes  \n     99.1431       -0.2926  \n\n#fit the model by all\nlm2a <- engine1 %>%  fit(BodyTemp~., data=fit1)\nlm2a\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = BodyTemp ~ ., data = data)\n\nCoefficients:\n           (Intercept)    SwollenLymphNodesYes      ChestCongestionYes  \n             97.942664               -0.166759                0.099264  \n       ChillsSweatsYes      NasalCongestionYes               SneezeYes  \n              0.191614               -0.220218               -0.365929  \n            FatigueYes      SubjectiveFeverYes             HeadacheYes  \n              0.267777                0.444491                0.006544  \n          WeaknessMild        WeaknessModerate          WeaknessSevere  \n              0.010970                0.094045                0.360500  \n    CoughIntensityMild  CoughIntensityModerate    CoughIntensitySevere  \n              0.344139                0.243298                0.265556  \n           MyalgiaMild         MyalgiaModerate           MyalgiaSevere  \n              0.162851               -0.029139               -0.134656  \n          RunnyNoseYes               AbPainYes            ChestPainYes  \n             -0.070649                0.023299                0.107697  \n           DiarrheaYes                EyePnYes             InsomniaYes  \n             -0.151106                0.123979               -0.007139  \n           ItchyEyeYes               NauseaYes                EarPnYes  \n             -0.002265               -0.034987                0.097256  \n            HearingYes          PharyngitisYes           BreathlessYes  \n              0.228115                0.310787                0.089794  \n            ToothPnYes               VisionYes                VomitYes  \n             -0.028160               -0.264254                0.160091  \n             WheezeYes  \n             -0.038518  \n\nglance(lm1a)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0123        0.0110  1.19      9.08 0.00268     1 -1162. 2329. 2343.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(lm2a)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.127        0.0851  1.14      3.06 0.0000000424    33 -1117. 2304. 2464.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n#logit model by RunnyNose\nengine2<- logistic_reg() %>%\n  set_engine(\"glm\") \nlogit1a <- engine2 %>%  fit(Nausea~RunnyNose, data=fit1)\n\n#logit model by all\nlogit2a <- engine2 %>%  fit(Nausea~., data=fit1)\n\nglance(logit1a)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -472.  949.  958.     945.         728   730\n\nglance(logit2a)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          945.     729  -376.  820.  976.     752.         696   730\n\n\nComparing\n\ncompare_performance(lm1a,lm2a)\n\n# Comparison of Model Performance Indices\n\nName | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2 | R2 (adj.) |  RMSE | Sigma\n---------------------------------------------------------------------------------------------------\nlm1a |   _lm | 2329.3 (<.001) | 2329.4 (<.001) | 2343.1 (>.999) | 0.012 |     0.011 | 1.188 | 1.190\nlm2a |   _lm | 2303.6 (>.999) | 2307.3 (>.999) | 2464.4 (<.001) | 0.127 |     0.085 | 1.117 | 1.144\n\ncompare_performance(logit1a,logit2a)\n\n# Comparison of Model Performance Indices\n\nName    | Model | AIC (weights) | AICc (weights) | BIC (weights) | Tjur's R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical |   PCP\n---------------------------------------------------------------------------------------------------------------------------------------------\nlogit1a |  _glm | 948.6 (<.001) |  948.6 (<.001) | 957.8 (>.999) | 1.169e-04 | 0.477 | 1.139 |    0.647 |  -107.871 |           0.012 | 0.545\nlogit2a |  _glm | 819.5 (>.999) |  823.0 (>.999) | 975.7 (<.001) |     0.247 | 0.414 | 1.039 |    0.515 |      -Inf |           0.002 | 0.658\n\n\nBoth comparisions shows that having all variables are better than having only main."
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "machinelearning",
    "section": "",
    "text": "Setup\n#Load package\n\n#load package\nlibrary(tidymodels)# for the tune package, along with the rest of tidymodels\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n\n✔ broom        1.0.4     ✔ recipes      1.0.5\n✔ dials        1.1.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.1     ✔ tibble       3.2.1\n✔ ggplot2      3.4.1     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.2     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rpart.plot)  # for visualizing a decision tree\n\nLoading required package: rpart\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(vip) # for variable importance plots\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(rpart)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-7\n\nlibrary(ranger)\nlibrary(ggplot2)\nlibrary(ggpubr)\n\n#Load data\n\nd1 <- readRDS(here::here('fluanalysis','data','processed_data.rds'))\n\n#setup\n\nset.seed(123)\n\nd1$Weakness <- factor(d1$Weakness, levels = c(\"None\",\"Mild\",\"Moderate\",\"Severe\"), ordered = TRUE)\nd1$CoughIntensity <- factor(d1$CoughIntensity, levels = c(\"None\",\"Mild\",\"Moderate\",\"Severe\"), ordered = TRUE)\nd1$Myalgia <- factor(d1$Myalgia, levels = c(\"None\",\"Mild\",\"Moderate\",\"Severe\"), ordered = TRUE)\n\n\n# Put 0.7 of the data into the training set \ndata_split <- initial_split(d1, prop = 0.7,strata = BodyTemp)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# Create 5-fold cross-validation, 5 times repeated\nCV5 <- vfold_cv(train_data, v = 5, repeats = 5, strata = BodyTemp)\n\n\n\n# Create a recipe\n# after checking is.order(), rebuild 3 variables.\nrec1 <- recipe(BodyTemp~.,data=train_data) %>%\nstep_dummy(all_nominal_predictors(),-Weakness,-CoughIntensity, -Myalgia) %>%\nstep_ordinalscore(Weakness, CoughIntensity, Myalgia)\n\n#Null performance\n\n# Null model performance\nnull_setup <- null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"regression\")\n\nnull_wf_train <- workflow() %>% \n  add_recipe(rec1)\n\nnull_model_train <- fit_resamples(null_wf_train %>% \n                                    add_model(null_setup), CV5,\n                                  metrics = metric_set(rmse))\n\n\nnull_model_train %>% collect_metrics() # 1.21\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    1.21    25  0.0177 Preprocessor1_Model1\n\nnull_rmse <- null_model_train %>% collect_metrics()\n\nresult_location1 = here(\"fluanalysis\",\"results\", \"null_rmse.rds\")\nsaveRDS(null_rmse, file = result_location1)\n\n#Model tuning and fitting ##Specifiation\n\n#Specification\n#Decision tree\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n# LASSO\nglm_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n# Random forest\ncores <- parallel::detectCores()\ncores\n\n[1] 12\n\nrf_spec <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"regression\")\n\n##Work flow\n\n#Work flow\n#Decision tree\ntree_wf <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(tune_spec)\n#LASSO\nglm_wf=workflow()%>%\n  add_recipe(rec1)%>%\n  add_model(glm_spec)\n#Random forest\nrf_wf=workflow()%>%\n  add_recipe(rec1)%>%\n  add_model(rf_spec)\n\n##Grid\n\n#Decision tree\n  tree_grid <- grid_regular(cost_complexity(),\n                            tree_depth(),\n                            levels = 5)\n  tree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n#LASSO\n  glm_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n  glm_grid %>% top_n(-5) # lowest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\n  glm_grid %>% top_n(5)  # highest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n\n#Random forest\n  extract_parameter_set_dials(rf_spec)\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\n##cv\n\n  #Decision tree\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n  resamples = CV5,\n  grid = tree_grid\n  )\n\n! Fold1, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat1: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat2: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat3: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat4: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold1, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold2, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold3, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold4, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\n\n! Fold5, Repeat5: internal:\n  There was 1 warning in `dplyr::summarise()`.\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 1`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 4`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 8`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 11`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n  ℹ In argument: `.estimate = metric_fn(truth = BodyTemp, estimate = .pr...\n    = na_rm)`.\n  ℹ In group 1: `cost_complexity = 0.1`, `tree_depth = 15`.\n  Caused by warning:\n  ! A correlation computation is required, but `estimate` is constant an...\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times using stratification \n# A tibble: 25 × 5\n   splits            id      id2   .metrics          .notes          \n   <list>            <chr>   <chr> <list>            <list>          \n 1 <split [405/103]> Repeat1 Fold1 <tibble [50 × 6]> <tibble [1 × 3]>\n 2 <split [405/103]> Repeat1 Fold2 <tibble [50 × 6]> <tibble [1 × 3]>\n 3 <split [406/102]> Repeat1 Fold3 <tibble [50 × 6]> <tibble [1 × 3]>\n 4 <split [408/100]> Repeat1 Fold4 <tibble [50 × 6]> <tibble [1 × 3]>\n 5 <split [408/100]> Repeat1 Fold5 <tibble [50 × 6]> <tibble [1 × 3]>\n 6 <split [405/103]> Repeat2 Fold1 <tibble [50 × 6]> <tibble [1 × 3]>\n 7 <split [405/103]> Repeat2 Fold2 <tibble [50 × 6]> <tibble [1 × 3]>\n 8 <split [406/102]> Repeat2 Fold3 <tibble [50 × 6]> <tibble [1 × 3]>\n 9 <split [408/100]> Repeat2 Fold4 <tibble [50 × 6]> <tibble [1 × 3]>\n10 <split [408/100]> Repeat2 Fold5 <tibble [50 × 6]> <tibble [1 × 3]>\n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x25: There was 1 warning in `dplyr::summarise()`. ℹ In argument: `.est...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n#LASSO\nglm_res <-glm_wf%>%\ntune_grid(\n  resamples=CV5,\n  grid=glm_grid,\n  control=control_grid(save_pred = TRUE),\n  metrics=metric_set(rmse)\n  )\nglm_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times using stratification \n# A tibble: 25 × 6\n   splits            id      id2   .metrics          .notes   .predictions\n   <list>            <chr>   <chr> <list>            <list>   <list>      \n 1 <split [405/103]> Repeat1 Fold1 <tibble [30 × 5]> <tibble> <tibble>    \n 2 <split [405/103]> Repeat1 Fold2 <tibble [30 × 5]> <tibble> <tibble>    \n 3 <split [406/102]> Repeat1 Fold3 <tibble [30 × 5]> <tibble> <tibble>    \n 4 <split [408/100]> Repeat1 Fold4 <tibble [30 × 5]> <tibble> <tibble>    \n 5 <split [408/100]> Repeat1 Fold5 <tibble [30 × 5]> <tibble> <tibble>    \n 6 <split [405/103]> Repeat2 Fold1 <tibble [30 × 5]> <tibble> <tibble>    \n 7 <split [405/103]> Repeat2 Fold2 <tibble [30 × 5]> <tibble> <tibble>    \n 8 <split [406/102]> Repeat2 Fold3 <tibble [30 × 5]> <tibble> <tibble>    \n 9 <split [408/100]> Repeat2 Fold4 <tibble [30 × 5]> <tibble> <tibble>    \n10 <split [408/100]> Repeat2 Fold5 <tibble [30 × 5]> <tibble> <tibble>    \n# ℹ 15 more rows\n\n#Random forest\n\nrf_res <- rf_wf %>%\n  tune_grid(\n    resamples=CV5,\n    grid=25,\n    control=control_grid(save_pred=TRUE),\n    metrics = metric_set(rmse))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times using stratification \n# A tibble: 25 × 6\n   splits            id      id2   .metrics          .notes   .predictions\n   <list>            <chr>   <chr> <list>            <list>   <list>      \n 1 <split [405/103]> Repeat1 Fold1 <tibble [25 × 6]> <tibble> <tibble>    \n 2 <split [405/103]> Repeat1 Fold2 <tibble [25 × 6]> <tibble> <tibble>    \n 3 <split [406/102]> Repeat1 Fold3 <tibble [25 × 6]> <tibble> <tibble>    \n 4 <split [408/100]> Repeat1 Fold4 <tibble [25 × 6]> <tibble> <tibble>    \n 5 <split [408/100]> Repeat1 Fold5 <tibble [25 × 6]> <tibble> <tibble>    \n 6 <split [405/103]> Repeat2 Fold1 <tibble [25 × 6]> <tibble> <tibble>    \n 7 <split [405/103]> Repeat2 Fold2 <tibble [25 × 6]> <tibble> <tibble>    \n 8 <split [406/102]> Repeat2 Fold3 <tibble [25 × 6]> <tibble> <tibble>    \n 9 <split [408/100]> Repeat2 Fold4 <tibble [25 × 6]> <tibble> <tibble>    \n10 <split [408/100]> Repeat2 Fold5 <tibble [25 × 6]> <tibble> <tibble>    \n# ℹ 15 more rows\n\n\n#Evaluation ##Decision tree\n\n#Decision tree\n#plot the result\ntree_res %>% autoplot()\n\n\n\n#choose best tree\nbest_tree <- tree_res %>%\n  select_best(\"rmse\")\n#finalizing the model\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\nfinal_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 1\n\nComputational engine: rpart \n\n#fit by train data\nfinal_fit <- \n  final_wf %>%\n  fit(train_data) \n#predict value\ntree_predict <- final_fit %>%\n  predict(train_data)\n#actual vs predict\ntree_plot <- as.data.frame(cbind(train_data$BodyTemp,tree_predict$`.pred`))\nnames(tree_plot)[1:2] <- c('actual','predict')\n\ntree_plot1 <- pivot_longer(tree_plot,colnames(tree_plot)) \nplot(tree_plot,'actual','predict')\n\n\n\nboxplot(tree_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_tree <- train_data$BodyTemp-tree_predict\nplot(tree_predict$.pred,residual_tree$.pred)\n\n\n\nboxplot(as.factor(tree_predict$.pred),residual_tree$.pred,names=c('98.69','99.23'))\n\n\n\n#levels(as.factor(tree_predict$.pred))\n\n##LASSO\n\n#LASSO\n# plot the result\nglm_res%>%\n  autoplot()\n\n\n\n# Show the best\nglm_res%>%\n  show_best(\"rmse\")\n\n# A tibble: 5 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1  0.0489 rmse    standard    1.15    25  0.0171 Preprocessor1_Model27\n2  0.0621 rmse    standard    1.15    25  0.0170 Preprocessor1_Model28\n3  0.0386 rmse    standard    1.15    25  0.0171 Preprocessor1_Model26\n4  0.0788 rmse    standard    1.15    25  0.0171 Preprocessor1_Model29\n5  0.0304 rmse    standard    1.15    25  0.0172 Preprocessor1_Model25\n\n# Choose the best\nglm_best <- \n  glm_res %>% \n  select_best(metric = \"rmse\")\nglm_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0489 Preprocessor1_Model27\n\n# Finalize the workflow\nglm_final <- \n  glm_wf%>%\n  finalize_workflow(glm_best)\n#fit by train data\nglm_fit <- \n  glm_final %>%\n  fit(train_data) \n#predict value\nglm_predict <- glm_fit%>%\n  predict(train_data)\n\n#actual vs predict\nglm_plot <- as.data.frame(cbind(train_data$BodyTemp,glm_predict$`.pred`))\nnames(glm_plot)[1:2] <- c('actual','predict')\n\nglm_plot1 <- pivot_longer(glm_plot,colnames(glm_plot)) \nggboxplot(glm_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(glm_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_glm <- train_data$BodyTemp-glm_predict\nplot(glm_predict$.pred,residual_glm$.pred)\n\n\n\n\n##Random forest\n\n#random forest\n# plot the result\nrf_res%>%\n  autoplot()\n\n\n\n# Show the best\nrf_res%>%\n  show_best(\"rmse\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     5    38 rmse    standard    1.16    25  0.0167 Preprocessor1_Model01\n2     4    34 rmse    standard    1.16    25  0.0168 Preprocessor1_Model25\n3     2     2 rmse    standard    1.16    25  0.0171 Preprocessor1_Model21\n4     2    25 rmse    standard    1.16    25  0.0171 Preprocessor1_Model06\n5     8    39 rmse    standard    1.16    25  0.0169 Preprocessor1_Model05\n\n# Choose the best\nrf_best <- \n  rf_res %>% \n  select_best(metric = \"rmse\")\nrf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     5    38 Preprocessor1_Model01\n\n# Finalize the workflow\nrf_final <- \n rf_wf%>%\n  finalize_workflow(rf_best)\n#fit by train data\nrf_fit <- \n  rf_final %>%\n  fit(train_data) \n#predict value\nrf_predict <- rf_fit%>%\n  predict(train_data)\n\n#actual vs predict\nrf_plot <- as.data.frame(cbind(train_data$BodyTemp,rf_predict$`.pred`))\nnames(rf_plot)[1:2] <- c('actual','predict')\n\nrf_plot1 <- pivot_longer(rf_plot,colnames(rf_plot)) \nggboxplot(rf_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(rf_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_rf <- train_data$BodyTemp-rf_predict\nplot(rf_predict$.pred,residual_rf$.pred)\n\n\n\n\n#Null model compare\n\n# compare model performance\ntree_rmse <- tree_res%>%\n  show_best(\"rmse\")%>%\n  select(3:8)\n\nglm_rmse <- glm_res%>%\n  show_best(\"rmse\")%>%\n  select(2:7)\n\nrf_rmse <- rf_res%>%\n  show_best(\"rmse\")%>%\n  select(3:8)\n\nnull_rmse <- null_model_train %>% \n  show_best(\"rmse\")\n\nmodel <- c('null','tree','lasso','random forest')\n\ncomparision <- rbind(null_rmse[1,],tree_rmse[1,],glm_rmse[1,],rf_rmse[1,])\n\ncomparision1 <- cbind(comparision,model)\n\ncomparision1\n\n  .metric .estimator     mean  n    std_err               .config         model\n1    rmse   standard 1.206661 25 0.01767230  Preprocessor1_Model1          null\n2    rmse   standard 1.189406 25 0.01806070 Preprocessor1_Model01          tree\n3    rmse   standard 1.151395 25 0.01706500 Preprocessor1_Model27         lasso\n4    rmse   standard 1.157735 25 0.01674963 Preprocessor1_Model01 random forest\n\n#lasso have lowest rmse\n\nn <- nrow(train_data)\nupper_chisq <- qchisq(0.025, n, lower.tail=FALSE)\nlower_chisq <- qchisq(0.025, n, lower.tail=TRUE)\n\n#95CI for each model\n\ncomparision1$lower <- comparision1$mean*sqrt(n/upper_chisq)\ncomparision1$upper <- comparision1$mean*sqrt(n/lower_chisq)\ncomparision1$`95%CI` <- paste('(',comparision1$lower,',',comparision1$upper,')')\ncomparision1\n\n  .metric .estimator     mean  n    std_err               .config         model\n1    rmse   standard 1.206661 25 0.01767230  Preprocessor1_Model1          null\n2    rmse   standard 1.189406 25 0.01806070 Preprocessor1_Model01          tree\n3    rmse   standard 1.151395 25 0.01706500 Preprocessor1_Model27         lasso\n4    rmse   standard 1.157735 25 0.01674963 Preprocessor1_Model01 random forest\n     lower    upper                                   95%CI\n1 1.136810 1.285727 ( 1.13681020647467 , 1.28572703409415 )\n2 1.120554 1.267342 ( 1.12055440689772 , 1.26734180069471 )\n3 1.084744 1.226840 ( 1.08474387228892 , 1.22684025330389 )\n4 1.090716 1.233595  ( 1.0907162116404 , 1.23359494121683 )\n\n#so, 95% CI of rmse for LASSO looks the best. \n\nLASSO has the best 95%CI of RMSE, so here I choose LASSO\n#Final evaluation\n\n#final evaluation\n\nglm_last_fit <- glm_final%>%\n  last_fit(data_split)\nglm_last_fit%>%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      1.16   Preprocessor1_Model1\n2 rsq     standard      0.0278 Preprocessor1_Model1\n\nglm_predict_final <- glm_last_fit%>%\n  collect_predictions()\nglm_last_fit%>%\n  extract_fit_engine()%>%\n  vip()\n\n\n\n#performance RMSE=1.16 RSQ=0.0312\n\n#actual vs predict\n\nnames(glm_predict_final)[c(4,2)] <- c('actual','predict')\n\nglm_final_plot1 <- pivot_longer(glm_predict_final,c('actual','predict')) \nggboxplot(glm_final_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(as.numeric(glm_predict_final$actual),as.numeric(glm_predict_final$predict))\n\n\n\n#residual plot\nresidual_glm_final <- glm_predict_final$actual-glm_predict_final$predict\nplot(glm_predict_final$predict,residual_glm_final)"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "modeleval",
    "section": "",
    "text": "Setup\nLoad package\n\n#load package\nlibrary(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\nlibrary(skimr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidymodels) \n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\n\nLoad data\n\nd1 <- readRDS(here::here('fluanalysis','data','processed_data.rds'))\n\nFit the model use all predictor\n\n# Fix the random numbers by setting the seed \n# This enables the analysis to be reproducible when random numbers are used \nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(d1, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# Initial a new recipe\nNausea_rec <- \n  recipe(Nausea ~ ., data = train_data) \n\n# Fit a model with a recipe\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nNausea_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(Nausea_rec)\n\nNausea_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nNausea_fit <- \n  Nausea_wflow %>% \n  fit(data = train_data)\n\nNausea_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 34 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)            2.43       9.37      0.259  0.795 \n 2 SwollenLymphNodesYes  -0.243      0.232    -1.05   0.293 \n 3 ChestCongestionYes     0.171      0.252     0.678  0.498 \n 4 ChillsSweatsYes        0.137      0.331     0.413  0.680 \n 5 NasalCongestionYes     0.588      0.310     1.90   0.0578\n 6 SneezeYes              0.115      0.248     0.463  0.643 \n 7 FatigueYes             0.178      0.439     0.407  0.684 \n 8 SubjectiveFeverYes     0.213      0.263     0.812  0.417 \n 9 HeadacheYes            0.464      0.351     1.32   0.186 \n10 WeaknessMild          -0.0599     0.510    -0.117  0.907 \n# ℹ 24 more rows\n\n\nPredict\n\n#Use a trained workflow to predict\n#train_data\npredict(Nausea_fit, train_data)\n\n# A tibble: 547 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 Yes        \n 3 No         \n 4 No         \n 5 Yes        \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# ℹ 537 more rows\n\nNausea_aug <- \n  augment(Nausea_fit, train_data)\n\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at <https://github.com/tidymodels/yardstick/issues>.\n\n\n\n\n#get the value of ROC\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No)\n\n# A tibble: 548 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1  -Inf          0                 1\n 2     0.0113     0                 1\n 3     0.0152     0.00535           1\n 4     0.0158     0.0107            1\n 5     0.0160     0.0160            1\n 6     0.0205     0.0214            1\n 7     0.0216     0.0267            1\n 8     0.0252     0.0321            1\n 9     0.0272     0.0374            1\n10     0.0280     0.0428            1\n# ℹ 538 more rows\n\n#Estimates the area under the curve\nNausea_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.786\n\n#test_data\npredict(Nausea_fit, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 Yes        \n 7 Yes        \n 8 No         \n 9 No         \n10 Yes        \n# ℹ 173 more rows\n\nNausea_aug <- \n  augment(Nausea_fit, test_data)\n\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\n#get the value of ROC\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No)\n\n# A tibble: 185 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1  -Inf           0            1    \n 2     0.0332      0            1    \n 3     0.0532      0.0147       1    \n 4     0.0937      0.0294       1    \n 5     0.115       0.0441       1    \n 6     0.116       0.0588       1    \n 7     0.141       0.0735       1    \n 8     0.145       0.0735       0.991\n 9     0.162       0.0882       0.991\n10     0.164       0.0882       0.983\n# ℹ 175 more rows\n\n#Estimates the area under the curve\nNausea_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.730\n\n\nFit the model only use main predictor\n\n#Alternative model\n# This enables the analysis to be reproducible when random numbers are used \nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(d1, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# Initial a new recipe\nNausea_rec <- \n  recipe(Nausea ~ RunnyNose, data = train_data) \n\n# Fit a model with a recipe\nlr_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nNausea_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(Nausea_rec)\n\nNausea_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nNausea_fit <- \n  Nausea_wflow %>% \n  fit(data = train_data)\n\nNausea_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic    p.value\n  <chr>           <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)    -0.790     0.172    -4.59  0.00000447\n2 RunnyNoseYes    0.188     0.202     0.930 0.352     \n\n#Use a trained workflow to predict \n#train_data\npredict(Nausea_fit, train_data)\n\n# A tibble: 547 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# ℹ 537 more rows\n\nNausea_aug <- \n  augment(Nausea_fit, train_data)\n\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\n#get the value of ROC\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No)\n\n# A tibble: 4 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1   -Inf           0             1  \n2      0.646       0             1  \n3      0.688       0.738         0.3\n4    Inf           1             0  \n\n#Estimates the area under the curve\nNausea_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.519\n\n#test_data\npredict(Nausea_fit, test_data)\n\n# A tibble: 183 × 1\n   .pred_class\n   <fct>      \n 1 No         \n 2 No         \n 3 No         \n 4 No         \n 5 No         \n 6 No         \n 7 No         \n 8 No         \n 9 No         \n10 No         \n# ℹ 173 more rows\n\nNausea_aug <- \n  augment(Nausea_fit, test_data)\n\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No) %>% \n  autoplot()\n\n\n\n#get the value of ROC\nNausea_aug %>% \n  roc_curve(truth = Nausea, .pred_No)\n\n# A tibble: 4 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1   -Inf           0           1    \n2      0.646       0           1    \n3      0.688       0.662       0.270\n4    Inf           1           0    \n\n#Estimates the area under the curve\nNausea_aug %>% \n  roc_auc(truth = Nausea, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.466\n\n############################################################\n########### This section added by NICOLE LUISI #############\n############################################################\n\n# Prep new testing and training sets\nset.seed(222)\ndata_split_part2 <- initial_split(d1, prop = 3/4)\ntrain_data_part2 <- training(data_split_part2)\ntest_data_part2  <- testing(data_split_part2)\n\n# Recipe\nrec_part2 <- recipe(BodyTemp ~ ., data = train_data_part2) \n\n# Set engine\nln_mod_part2 <- linear_reg() %>% set_engine(\"lm\")\n\n# Workflow\nwflow_part2 <- \n  workflow() %>% \n  add_model(ln_mod_part2) %>% \n  add_recipe(rec_part2)\n\n# Prepare recipe and train model \nmod10_fit_part2 <- wflow_part2 %>% fit(data = train_data_part2)\n\n# Pull fitted model object, get model coefficients\nmod10_fit_part2 %>% extract_fit_parsnip() %>% tidy()\n\n# A tibble: 34 × 5\n   term                 estimate std.error statistic    p.value\n   <chr>                   <dbl>     <dbl>     <dbl>      <dbl>\n 1 (Intercept)           97.7        0.344   284.    0         \n 2 SwollenLymphNodesYes  -0.189      0.108    -1.75  0.0814    \n 3 ChestCongestionYes     0.146      0.115     1.26  0.207     \n 4 ChillsSweatsYes        0.186      0.148     1.25  0.210     \n 5 NasalCongestionYes    -0.179      0.136    -1.31  0.190     \n 6 SneezeYes             -0.473      0.114    -4.15  0.0000392 \n 7 FatigueYes             0.361      0.187     1.93  0.0544    \n 8 SubjectiveFeverYes     0.563      0.119     4.74  0.00000277\n 9 HeadacheYes            0.0689     0.151     0.457 0.648     \n10 WeaknessMild           0.0769     0.212     0.363 0.716     \n# ℹ 24 more rows\n\n# Use the trained workflow to predict with the unseen test data\noptions(warn=-1)\npredict(mod10_fit_part2, test_data_part2)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.3\n 2  99.0\n 3  99.8\n 4  98.7\n 5  99.0\n 6  99.5\n 7  99.2\n 8  98.8\n 9  99.5\n10  98.8\n# ℹ 173 more rows\n\nmod10_aug_part2 <- augment(mod10_fit_part2, test_data_part2)\nmod10_aug_part2 %>% select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.3\n 2     98.8  99.0\n 3    102.   99.8\n 4     98.2  98.7\n 5     97.8  99.0\n 6     97.8  99.5\n 7    100    99.2\n 8    101.   98.8\n 9     98.8  99.5\n10    100.   98.8\n# ℹ 173 more rows\n\noptions(warn=1) \n\n# Model fit with RMSE\nmod10_aug_part2 %>% rmse(truth = BodyTemp, .pred) # 1.15\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.15\n\n# Model fit with R^2\nmod10_aug_part2 %>% rsq(truth = BodyTemp, .pred) #0.05\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard      0.0456\n\n# Model with main predictor\n\n# Recipe\nrec_part2b <- recipe(BodyTemp ~ RunnyNose, data = train_data_part2) \n\n# Set engine\nln_mod_part2b <- linear_reg() %>% set_engine(\"lm\")\n\n# Workflow\nwflow_part2b <- \n  workflow() %>% \n  add_model(ln_mod_part2b) %>% \n  add_recipe(rec_part2b)\n\n# Prepare recipe and train model \nmod10_fit_part2b <- wflow_part2b %>% fit(data = train_data_part2)\n\n# Pull fitted model object, get model coefficients\nmod10_fit_part2b %>% extract_fit_parsnip() %>% tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0964   1028.    0     \n2 RunnyNoseYes   -0.261    0.114      -2.29  0.0225\n\n# Use the trained workflow to predict with the unseen test data\npredict(mod10_fit_part2b, test_data_part2)\n\n# A tibble: 183 × 1\n   .pred\n   <dbl>\n 1  99.1\n 2  98.9\n 3  98.9\n 4  98.9\n 5  99.1\n 6  99.1\n 7  98.9\n 8  99.1\n 9  99.1\n10  99.1\n# ℹ 173 more rows\n\nmod10_aug_part2b <- augment(mod10_fit_part2b, test_data_part2)\nmod10_aug_part2b %>% select(BodyTemp, .pred)\n\n# A tibble: 183 × 2\n   BodyTemp .pred\n      <dbl> <dbl>\n 1     98.3  99.1\n 2     98.8  98.9\n 3    102.   98.9\n 4     98.2  98.9\n 5     97.8  99.1\n 6     97.8  99.1\n 7    100    98.9\n 8    101.   99.1\n 9     98.8  99.1\n10    100.   99.1\n# ℹ 173 more rows\n\n# Model fit with RMSE\nmod10_aug_part2b %>% rmse(truth = BodyTemp, .pred) # 1.13\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.13\n\n# Model fit with R^2\nmod10_aug_part2b %>% rsq(truth = BodyTemp, .pred) # 0.24\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard      0.0240\n\n\nPredict in alternative model\n\n\n\n\n\nSummary\nIn the model using all predictors, area under the curve is 0.787 for train_data. It’s a fine model. For test_data, that’s 0.724. It’s fine as well.\nIn the model using the main predictor, area under the curve is 0.519 for train_data. So, this isn’t a good model. For test_data, that’s 0.466. This indicate that isn’t a good model.\n\n\nThis section added by NICOLE LUISI\nNote: Following setup started by Yao, the code for part 2 was added to the modeleval.R script in the same folder that is called at the beginning of this Quarto file.\nSummary: Model with all predictors is better. The RMSE is similar here, but the model with all of the predictors has a higher r-squared."
  },
  {
    "objectID": "fluanalysis/code/wrangling.html",
    "href": "fluanalysis/code/wrangling.html",
    "title": "wrangling",
    "section": "",
    "text": "library(here)\n\nhere() starts at E:/uga course/bios8060e/res1/yaolu-MADA-portfolio\n\nlibrary(skimr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n#load data\nt1 <- readRDS(here::here('fluanalysis','data','SympAct_Any_Pos.Rda'))\nhead(t1)\n\n                               DxName1\n1 Influenza like illness - Clinical Dx\n2       Acute tonsillitis, unspecified\n3 Influenza like illness - Clinical Dx\n4 Influenza like illness - Clinical Dx\n5       Acute pharyngitis, unspecified\n6 Influenza like illness - Clinical Dx\n                                       DxName2 DxName3 DxName4 DxName5\n1                                         <NA>    <NA>    <NA>    <NA>\n2         Influenza like illness - Clinical Dx    <NA>    <NA>    <NA>\n3               Acute pharyngitis, unspecified    <NA>    <NA>    <NA>\n4 Unspecified asthma with (acute) exacerbation    <NA>    <NA>    <NA>\n5         Influenza like illness - Clinical Dx    <NA>    <NA>    <NA>\n6                                         <NA>    <NA>    <NA>    <NA>\n  Unique.Visit ActivityLevel ActivityLevelF SwollenLymphNodes ChestCongestion\n1 340_17632125            10             10               Yes              No\n2 340_17794836             6              6               Yes             Yes\n3 342_17737773             2              2               Yes             Yes\n4 342_17806002             2              2               Yes             Yes\n5 342_17610918             5              5               Yes              No\n6 343_17543967             3              3                No              No\n  ChillsSweats NasalCongestion CoughYN Sneeze Fatigue SubjectiveFever Headache\n1           No              No     Yes     No     Yes             Yes      Yes\n2           No             Yes     Yes     No     Yes             Yes      Yes\n3          Yes             Yes      No    Yes     Yes             Yes      Yes\n4          Yes             Yes     Yes    Yes     Yes             Yes      Yes\n5          Yes              No      No     No     Yes             Yes      Yes\n6          Yes              No     Yes    Yes     Yes             Yes      Yes\n  Weakness WeaknessYN CoughIntensity CoughYN2  Myalgia MyalgiaYN RunnyNose\n1     Mild        Yes         Severe      Yes     Mild       Yes        No\n2   Severe        Yes         Severe      Yes   Severe       Yes        No\n3   Severe        Yes           Mild      Yes   Severe       Yes       Yes\n4   Severe        Yes       Moderate      Yes   Severe       Yes       Yes\n5 Moderate        Yes           None       No     Mild       Yes        No\n6 Moderate        Yes       Moderate      Yes Moderate       Yes        No\n  AbPain ChestPain Diarrhea EyePn Insomnia ItchyEye Nausea EarPn Hearing\n1     No        No       No    No       No       No     No    No      No\n2     No        No       No    No       No       No     No   Yes     Yes\n3    Yes       Yes       No    No      Yes       No    Yes    No      No\n4     No        No       No    No      Yes       No    Yes   Yes      No\n5     No        No       No   Yes      Yes       No    Yes    No      No\n6     No       Yes      Yes    No       No       No    Yes    No      No\n  Pharyngitis Breathless ToothPn Vision Vomit Wheeze BodyTemp\n1         Yes         No      No     No    No     No     98.3\n2         Yes         No      No     No    No     No    100.4\n3         Yes        Yes     Yes     No    No     No    100.8\n4         Yes         No      No     No    No    Yes     98.8\n5         Yes         No      No     No    No     No    100.5\n6         Yes        Yes      No     No    No    Yes     98.4\n                             RapidFluA                            RapidFluB\n1 Presumptive Negative For Influenza A Presumptive Negative For Influenza B\n2                                 <NA>                                 <NA>\n3 Presumptive Negative For Influenza A Presumptive Negative For Influenza B\n4 Presumptive Negative For Influenza A Presumptive Negative For Influenza B\n5                                 <NA>                                 <NA>\n6                                 <NA>                                 <NA>\n  PCRFluA PCRFluB TransScore1 TransScore1F TransScore2 TransScore2F TransScore3\n1    <NA>    <NA>           1            1           1            1           1\n2    <NA>    <NA>           3            3           2            2           1\n3    <NA>    <NA>           4            4           3            3           2\n4    <NA>    <NA>           5            5           4            4           3\n5    <NA>    <NA>           0            0           0            0           0\n6    <NA>    <NA>           2            2           2            2           2\n  TransScore3F TransScore4 TransScore4F ImpactScore ImpactScore2 ImpactScore3\n1            1           0            0           7            6            3\n2            1           2            2           8            7            4\n3            2           4            4          14           13            9\n4            3           4            4          12           11            7\n5            0           0            0          11           10            6\n6            2           1            1          12           11            7\n  ImpactScoreF ImpactScore2F ImpactScore3F ImpactScoreFD TotalSymp1 TotalSymp1F\n1            7             6             3             7          8           8\n2            8             7             4             8         11          11\n3           14            13             9            14         18          18\n4           12            11             7            12         17          17\n5           11            10             6            11         11          11\n6           12            11             7            12         14          14\n  TotalSymp2 TotalSymp3\n1          8          8\n2         10          9\n3         17         16\n4         16         15\n5         11         11\n6         14         14\n\nskim(t1)\n\n\nData summary\n\n\nName\nt1\n\n\nNumber of rows\n735\n\n\nNumber of columns\n63\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n50\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nUnique.Visit\n0\n1\n10\n12\n0\n735\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDxName1\n0\n1.00\nFALSE\n25\nInf: 328, Inf: 131, Fev: 101, Cou: 66\n\n\nDxName2\n280\n0.62\nFALSE\n42\nInf: 126, Inf: 115, Fev: 45, Cou: 41\n\n\nDxName3\n626\n0.15\nFALSE\n37\nInf: 23, Inf: 14, Cou: 10, Fev: 6\n\n\nDxName4\n716\n0.03\nFALSE\n14\nInf: 3, Acu: 2, Enc: 2, Inf: 2\n\n\nDxName5\n734\n0.00\nFALSE\n1\nHea: 1, Acu: 0, Enc: 0, Oth: 0\n\n\nActivityLevelF\n0\n1.00\nFALSE\n11\n3: 125, 5: 97, 4: 95, 2: 80\n\n\nSwollenLymphNodes\n0\n1.00\nFALSE\n2\nNo: 421, Yes: 314\n\n\nChestCongestion\n0\n1.00\nFALSE\n2\nYes: 409, No: 326\n\n\nChillsSweats\n0\n1.00\nFALSE\n2\nYes: 604, No: 131\n\n\nNasalCongestion\n0\n1.00\nFALSE\n2\nYes: 565, No: 170\n\n\nCoughYN\n0\n1.00\nFALSE\n2\nYes: 660, No: 75\n\n\nSneeze\n0\n1.00\nFALSE\n2\nYes: 395, No: 340\n\n\nFatigue\n0\n1.00\nFALSE\n2\nYes: 671, No: 64\n\n\nSubjectiveFever\n0\n1.00\nFALSE\n2\nYes: 505, No: 230\n\n\nHeadache\n0\n1.00\nFALSE\n2\nYes: 620, No: 115\n\n\nWeakness\n0\n1.00\nFALSE\n4\nMod: 341, Mil: 224, Sev: 121, Non: 49\n\n\nWeaknessYN\n0\n1.00\nFALSE\n2\nYes: 686, No: 49\n\n\nCoughIntensity\n0\n1.00\nFALSE\n4\nMod: 360, Sev: 172, Mil: 156, Non: 47\n\n\nCoughYN2\n0\n1.00\nFALSE\n2\nYes: 688, No: 47\n\n\nMyalgia\n0\n1.00\nFALSE\n4\nMod: 327, Mil: 214, Sev: 115, Non: 79\n\n\nMyalgiaYN\n0\n1.00\nFALSE\n2\nYes: 656, No: 79\n\n\nRunnyNose\n0\n1.00\nFALSE\n2\nYes: 524, No: 211\n\n\nAbPain\n0\n1.00\nFALSE\n2\nNo: 642, Yes: 93\n\n\nChestPain\n0\n1.00\nFALSE\n2\nNo: 501, Yes: 234\n\n\nDiarrhea\n0\n1.00\nFALSE\n2\nNo: 636, Yes: 99\n\n\nEyePn\n0\n1.00\nFALSE\n2\nNo: 622, Yes: 113\n\n\nInsomnia\n0\n1.00\nFALSE\n2\nYes: 419, No: 316\n\n\nItchyEye\n0\n1.00\nFALSE\n2\nNo: 553, Yes: 182\n\n\nNausea\n0\n1.00\nFALSE\n2\nNo: 477, Yes: 258\n\n\nEarPn\n0\n1.00\nFALSE\n2\nNo: 573, Yes: 162\n\n\nHearing\n0\n1.00\nFALSE\n2\nNo: 705, Yes: 30\n\n\nPharyngitis\n0\n1.00\nFALSE\n2\nYes: 614, No: 121\n\n\nBreathless\n0\n1.00\nFALSE\n2\nNo: 438, Yes: 297\n\n\nToothPn\n0\n1.00\nFALSE\n2\nNo: 569, Yes: 166\n\n\nVision\n0\n1.00\nFALSE\n2\nNo: 716, Yes: 19\n\n\nVomit\n0\n1.00\nFALSE\n2\nNo: 656, Yes: 79\n\n\nWheeze\n0\n1.00\nFALSE\n2\nNo: 514, Yes: 221\n\n\nRapidFluA\n407\n0.45\nFALSE\n2\nPos: 169, Pre: 159\n\n\nRapidFluB\n407\n0.45\nFALSE\n2\nPre: 302, Pos: 26\n\n\nPCRFluA\n581\n0.21\nFALSE\n3\nIn: 120, In: 33, Ind: 1, Ass: 0\n\n\nPCRFluB\n581\n0.21\nFALSE\n2\nIn: 145, In: 9, Ass: 0\n\n\nTransScore1F\n0\n1.00\nFALSE\n6\n4: 210, 5: 195, 3: 157, 2: 107\n\n\nTransScore2F\n0\n1.00\nFALSE\n5\n4: 294, 3: 201, 2: 138, 1: 89\n\n\nTransScore3F\n0\n1.00\nFALSE\n4\n3: 323, 2: 222, 1: 166, 0: 24\n\n\nTransScore4F\n0\n1.00\nFALSE\n5\n3: 230, 4: 198, 2: 154, 1: 103\n\n\nImpactScoreF\n0\n1.00\nFALSE\n17\n8: 105, 9: 104, 10: 88, 7: 84\n\n\nImpactScore2F\n0\n1.00\nFALSE\n16\n7: 107, 8: 102, 9: 90, 10: 86\n\n\nImpactScore3F\n0\n1.00\nFALSE\n14\n4: 134, 5: 112, 3: 108, 6: 102\n\n\nImpactScoreFD\n0\n1.00\nFALSE\n17\n8: 105, 9: 104, 10: 88, 7: 84\n\n\nTotalSymp1F\n0\n1.00\nFALSE\n19\n12: 86, 13: 84, 14: 80, 11: 72\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nActivityLevel\n0\n1.00\n4.46\n2.64\n0.0\n3.0\n4.0\n6.0\n10.0\n▆▇▆▅▂\n\n\nBodyTemp\n5\n0.99\n98.94\n1.20\n97.2\n98.2\n98.5\n99.3\n103.1\n▇▇▂▁▁\n\n\nTransScore1\n0\n1.00\n3.47\n1.31\n0.0\n3.0\n4.0\n5.0\n5.0\n▂▅▆▇▇\n\n\nTransScore2\n0\n1.00\n2.92\n1.11\n0.0\n2.0\n3.0\n4.0\n4.0\n▁▂▃▆▇\n\n\nTransScore3\n0\n1.00\n2.15\n0.88\n0.0\n1.0\n2.0\n3.0\n3.0\n▁▅▁▆▇\n\n\nTransScore4\n0\n1.00\n2.58\n1.21\n0.0\n2.0\n3.0\n4.0\n4.0\n▂▃▆▇▇\n\n\nImpactScore\n0\n1.00\n9.51\n2.84\n2.0\n8.0\n9.0\n11.0\n18.0\n▂▇▇▅▁\n\n\nImpactScore2\n0\n1.00\n8.58\n2.78\n2.0\n7.0\n8.0\n10.0\n17.0\n▂▇▆▃▁\n\n\nImpactScore3\n0\n1.00\n5.06\n2.34\n0.0\n3.0\n5.0\n7.0\n13.0\n▂▇▃▂▁\n\n\nTotalSymp1\n0\n1.00\n12.99\n3.41\n5.0\n11.0\n13.0\n15.0\n23.0\n▂▇▇▅▁\n\n\nTotalSymp2\n0\n1.00\n12.43\n3.22\n4.0\n10.0\n12.0\n15.0\n22.0\n▁▇▇▅▁\n\n\nTotalSymp3\n0\n1.00\n11.66\n3.10\n3.0\n10.0\n12.0\n14.0\n21.0\n▁▇▇▅▁\n\n\n\n\n#remove columns contain these strings and remove NA\nt2 <- t1 %>%\n  dplyr::select(-contains(c(\"Score\", \"Total\", \"FluA\", \"FluB\", \"Dxname\", \"Activity\", \"Unique.Visit\"))) %>%\n  dplyr::select(-CoughYN,-WeaknessYN,-CoughYN2,-MyalgiaYN)%>%\n  drop_na()\n\n\nsummary(t2)\n\n SwollenLymphNodes ChestCongestion ChillsSweats NasalCongestion Sneeze   \n No :418           No :323         No :130      No :167         No :339  \n Yes:312           Yes:407         Yes:600      Yes:563         Yes:391  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Fatigue   SubjectiveFever Headache      Weakness    CoughIntensity\n No : 64   No :230         No :115   None    : 49   None    : 47   \n Yes:666   Yes:500         Yes:615   Mild    :223   Mild    :154   \n                                     Moderate:338   Moderate:357   \n                                     Severe  :120   Severe  :172   \n                                                                   \n                                                                   \n     Myalgia    RunnyNose AbPain    ChestPain Diarrhea  EyePn     Insomnia \n None    : 79   No :211   No :639   No :497   No :631   No :617   No :315  \n Mild    :213   Yes:519   Yes: 91   Yes:233   Yes: 99   Yes:113   Yes:415  \n Moderate:325                                                              \n Severe  :113                                                              \n                                                                           \n                                                                           \n ItchyEye  Nausea    EarPn     Hearing   Pharyngitis Breathless ToothPn  \n No :551   No :475   No :568   No :700   No :119     No :436    No :565  \n Yes:179   Yes:255   Yes:162   Yes: 30   Yes:611     Yes:294    Yes:165  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Vision    Vomit     Wheeze       BodyTemp     \n No :711   No :652   No :510   Min.   : 97.20  \n Yes: 19   Yes: 78   Yes:220   1st Qu.: 98.20  \n                               Median : 98.50  \n                               Mean   : 98.94  \n                               3rd Qu.: 99.30  \n                               Max.   :103.10  \n\nt3 <- t2 %>% dplyr::select(-Vision,-Hearing)\n\n#730 obs of 26 variables\nglimpse(t2)\n\nRows: 730\nColumns: 28\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …\n\n#save the rds\nsaveRDS(t2, file = here(\"fluanalysis\", \"data\", \"processed_data.rds\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Happy new year\n\nAnd happy new semester\nWelcome to my website and data analysis portfolio.\nTop right is the link to repo.\n\nPlease use the Menu Bar above to look around.\nHave fun!"
  },
  {
    "objectID": "tidytuesday_exercise.html",
    "href": "tidytuesday_exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\nlibrary('tidytuesdayR')\ntuesdata <- tidytuesdayR::tt_load('2023-02-14')\n\n--- Compiling #TidyTuesday Information for 2023-02-14 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `age_gaps.csv`\n\n\n--- Download complete ---\n\ntuesdata <- tidytuesdayR::tt_load(2023, week = 7)\n\n--- Compiling #TidyTuesday Information for 2023-02-14 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `age_gaps.csv`\n\n\n--- Download complete ---\n\nage_gaps <- tuesdata$age_gaps\n\nFirst we want to have a look at if there are some directors who generate multiple movies.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(forcats)\nresults <- age_gaps %>%\n  group_by(director) %>%\n  summarize(count = n_distinct(movie_name))\n\nresults1 <- results[order(-results$count),]\n\nbarplot(results1$count[1:3],names.arg=results1$director[1:3])\n\n\n\n\nHere we can see that, base on the number of movies, Woody Allen, Martin Scorsese and Steven Spielberg are the top 3 directors.\nHere we focus on thess three directors to explore our data.\nWe want to have a look at the age distribution among three directors.\n\nlibrary(tidyr)\ndf <- age_gaps[which(age_gaps$director %in% c('Woody Allen','Martin Scorsese','Steven Spielberg')),]\n\ndf1 <- gather(df,key='actorgroup',value='age',12:13) \n\ndf1 %>% \n    mutate(actorgroup=if_else(actorgroup=='actor_1_age','older','younger')) %>%\n    ggplot( aes(x=director, y=age,fill=actorgroup)) + \n    geom_boxplot()+ggtitle('age group by director')\n\n\n\n\nHere we can see that, director Woody Allen like to choose aged older actor. From here, looks like age difference is biggest. So we would like to have a look at the difference.\n\ndf %>%\n    ggplot( aes(x=director, y=age_difference)) + \n    geom_boxplot()\n\n\n\n\nHard to say if the age_difference are different among these directors. So, I want to do some stitistical test.\n\nlibrary(ggpubr)\nggboxplot(df,x = 'director', y = 'age_difference',\n          palette = \"jco\",exact=FALSE)+ \n  stat_compare_means()+ggtitle('age difference')\n\n\n\n\nSo, they are not all same. Then I can try tukeytest.\n\nTukeyHSD(aov(lm(df$age_difference~df$director)))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(df$age_difference ~ df$director))\n\n$`df$director`\n                                     diff         lwr      upr     p adj\nSteven Spielberg-Martin Scorsese 2.576923 -8.25508953 13.40894 0.8320509\nWoody Allen-Martin Scorsese      9.226923  0.05232155 18.40152 0.0484482\nWoody Allen-Steven Spielberg     6.650000 -3.32384339 16.62384 0.2479557\n\n\nSuggestion for script writer. Woody Allen prefer large age_difference than Martin Scorsese. So, if you have a script with large age_diffenence, go to talk to Woody Allen rather than Martin Scorsese. Woody Allen will have a higher chance to accept your script.\n\ndf2 <- filter(df1,actorgroup=='actor_1_age')\n  TukeyHSD(aov(lm(df2$age~df2$director)))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(df2$age ~ df2$director))\n\n$`df2$director`\n                                     diff        lwr      upr     p adj\nSteven Spielberg-Martin Scorsese 1.084615 -11.009175 13.17841 0.9740921\nWoody Allen-Martin Scorsese      8.784615  -1.458699 19.02793 0.1053042\nWoody Allen-Steven Spielberg     7.700000  -3.435656 18.83566 0.2241361\n\ndf3 <- filter(df1,actorgroup=='actor_2_age')\nTukeyHSD(aov(lm(df3$age~df3$director)))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = lm(df3$age ~ df3$director))\n\n$`df3$director`\n                                       diff       lwr      upr     p adj\nSteven Spielberg-Martin Scorsese -1.4923077 -8.702591 5.717975 0.8699312\nWoody Allen-Martin Scorsese      -0.4423077 -6.549342 5.664727 0.9830222\nWoody Allen-Steven Spielberg      1.0500000 -5.589046 7.689046 0.9217317\n\n\nHere we can see that, there is no difference for actor age. No matter how old is the actor. You probably have same chance to be accepted by either these three directors. Don’t worry your age, you have same chance as older or younger actor."
  },
  {
    "objectID": "tidytuesday_exercise2.html",
    "href": "tidytuesday_exercise2.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n✔ readr     2.1.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.4     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(ggpubr)\nlibrary(bonsai)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\neggproduction  <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/egg-production.csv')\n\nRows: 220 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): prod_type, prod_process, source\ndbl  (2): n_hens, n_eggs\ndate (1): observed_month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncagefreepercentages <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-11/cage-free-percentages.csv')\n\nRows: 96 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): source\ndbl  (2): percent_hens, percent_eggs\ndate (1): observed_month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter viewing the dictionary of this data. I was considering these questions as interesting questions. How can we get more table-egg? How can we get more high-quality egg?\nBased on our dataset, our interest of outcome is table-egg. Since I am not a egg-picky person. So, I will focus on all table egg.(Though sometime I will buy organic eggs.) From observed, in ‘cagefreepercentages’, there are many NA in percent_eggs. Also, these NA observations have a non-organized observed date. Here I need to make a decision that if I should choose the authenticity or integrity. In cagefreepercentages, source from paper has authenticity but lose integrity, since it miss some month. Source from computed has integrity but lose part of authenticity. In this initial, I decide to choose integrity. There are some reasons. First, this is initial part of the analysis, we need to explore more to get a good research question. Second, I glimpse most authenticity. Though there are some difference, there is not much difference between paper and computed. Third, the date from the paper is not well organized.\nThen we combine the data by observed_month.\n\n#observed the month\ncage <- cagefreepercentages %>% \n  filter(!is.na(percent_eggs)) %>%\n  select(observed_month,everything())\n\negg <- eggproduction %>%\n  left_join(cage,by=\"observed_month\")\n\n#check NA in new dataset\negg$observed_month[is.na(egg$percent_hens)]\n\n[1] \"2016-07-31\" \"2016-07-31\" \"2021-02-28\" \"2021-02-28\"\n\n\nHere we find that, there are two NA in percent_hens. Luckly, it’s the first two and last two observations. So, I just drop them and use the middle part of the data.Also we don’t need source.\n\negg1 <- egg[which(!is.na(egg$percent_hens)),] %>%\n  select(-c(source.x,source.y))\n\nvisualize the data. And here we generate two new variables ‘n_eggs_table eggs_housing’=‘n_eggs_table eggs_all’-‘n_eggs_table eggs_cage-free(organic)’-‘n_eggs_table eggs_cage-free(non-organic)’. Similar as ‘n_hens_table eggs_housing’.\n\negg2 <- egg1 %>%\npivot_wider(names_from=c(prod_type,prod_process),values_from = c(n_hens,n_eggs))%>%\n  mutate(`n_hens_table eggs_housing` = `n_hens_table eggs_all`-\n           `n_hens_table eggs_cage-free (non-organic)`-`n_hens_table eggs_cage-free (organic)`)%>%\n  mutate(`n_eggs_table eggs_housing` = `n_eggs_table eggs_all`-\n           `n_eggs_table eggs_cage-free (non-organic)`-`n_eggs_table eggs_cage-free (organic)`)\n\n\npivot_longer(egg2,cols = 4:13, names_to = \"type\", values_to = \"number\")%>%\n ggplot(aes(x = observed_month, y = number, group = type,color=type))+geom_line()\n\n\n\n\nBased on the absolute value plot. Looks like only n_eggs_table eggs_cage-free(non-organic) are increasing over the time. Other variables are just flucuate over time.\n\npivot_longer(egg2,cols = 2:3, names_to = \"type\", values_to = \"number\") %>%\nggplot(aes(x = observed_month, y = number, group = type,color=type))+geom_line()\n\n\n\n\nBased on the relative value plot. Here we can see both the percentage of cage_free eggs and hens are increasing. Almost exact same trend.Then our final interest of outcome is ‘n_eggs_table eggs_all’.\n\negg3 <- egg2 %>%\n  select(c(2:5,8,9))\n\negg4 <- cbind(egg3[,1:2],egg3[,3:6]/10^7)\n\nSince eggs_all=cage_free+housing, so there is 100% linear relationship. We don’t want that all in our model. But we want to see if the percentage influence our outcome. We can build percentage instead. For simplicity, we just remove these values. Similar as the number of hens.\nThen we went into machine learning part. Here we do null,linear,tree, LASSO, random forest models.\n\nset.seed(123)\n\n# Put 0.7 of the data into the training set \ndata_split <- initial_split(egg3, prop = 0.7,strata = 'n_eggs_table eggs_all')\n\nWarning: The number of observations in each quantile is below the recommended threshold of 20.\n• Stratification will use 2 breaks instead.\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# Create 5-fold cross-validation, 5 times repeated\nCV5 <- vfold_cv(train_data, v = 5, repeats = 5)\n\n# Create a recipe\n#is.ordered(d2$population1)\n\nrec1 <- recipe(`n_eggs_table eggs_all`~.,data=train_data) \n\nThis is our null model\n\n# Null model performance\nnull_setup <- null_model() %>%\n  set_engine(\"parsnip\") %>%\n  set_mode(\"regression\")\n\nnull_wf_train <- workflow() %>% \n  add_recipe(rec1)\n\nnull_model_train <- fit_resamples(null_wf_train %>% \n                                    add_model(null_setup), CV5,\n                                  metrics = metric_set(rmse))\n\n\nnull_model_train %>% collect_metrics() # 330199026\n\n# A tibble: 1 × 6\n  .metric .estimator       mean     n   std_err .config             \n  <chr>   <chr>           <dbl> <int>     <dbl> <chr>               \n1 rmse    standard   330199026.    25 14382660. Preprocessor1_Model1\n\n\nThis is our linear model\n\n# Linear Model\n#spec\nln_spec <- linear_reg() %>% set_engine(\"lm\")\n\n#wf\nln_wf <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(ln_spec)\n\nln_model_train <- fit_resamples(ln_wf,\n                                     CV5,\n                                  metrics = metric_set(rmse))\n\nln_rmse <- ln_model_train %>% collect_metrics() #rmse 90989668\n\nThese are tree,LASSO and random forest models.\n\n#Specification\n#Decision tree\ntune_spec <- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune()\n  ) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n# LASSO\nglm_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n# Random forest\ncores <- parallel::detectCores()\ncores\n\n[1] 12\n\nrf_spec <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores,importance = \"impurity\") %>% \n  set_mode(\"regression\")\n\n\n#Work flow\n#Decision tree\ntree_wf <- workflow() %>%\n  add_recipe(rec1) %>%\n  add_model(tune_spec)\n#LASSO\nglm_wf=workflow()%>%\n  add_recipe(rec1)%>%\n  add_model(glm_spec)\n#Random forest\nrf_wf=workflow()%>%\n  add_recipe(rec1)%>%\n  add_model(rf_spec)\n\n\n#Decision tree\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\ntree_grid\n\n# A tibble: 25 × 2\n   cost_complexity tree_depth\n             <dbl>      <int>\n 1    0.0000000001          1\n 2    0.0000000178          1\n 3    0.00000316            1\n 4    0.000562              1\n 5    0.1                   1\n 6    0.0000000001          4\n 7    0.0000000178          4\n 8    0.00000316            4\n 9    0.000562              4\n10    0.1                   4\n# ℹ 15 more rows\n\n#LASSO\nglm_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\nglm_grid %>% top_n(-5) # lowest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\nglm_grid %>% top_n(5)  # highest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n\n#Random forest\nextract_parameter_set_dials(rf_spec)\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\n\n#Decision tree\ntree_res <- \n  tree_wf %>% \n  tune_grid(\n    resamples = CV5,\n    grid = tree_grid\n  )\ntree_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times \n# A tibble: 25 × 5\n   splits         id      id2   .metrics          .notes          \n   <list>         <chr>   <chr> <list>            <list>          \n 1 <split [28/8]> Repeat1 Fold1 <tibble [50 × 6]> <tibble [0 × 3]>\n 2 <split [29/7]> Repeat1 Fold2 <tibble [50 × 6]> <tibble [0 × 3]>\n 3 <split [29/7]> Repeat1 Fold3 <tibble [50 × 6]> <tibble [0 × 3]>\n 4 <split [29/7]> Repeat1 Fold4 <tibble [50 × 6]> <tibble [0 × 3]>\n 5 <split [29/7]> Repeat1 Fold5 <tibble [50 × 6]> <tibble [0 × 3]>\n 6 <split [28/8]> Repeat2 Fold1 <tibble [50 × 6]> <tibble [0 × 3]>\n 7 <split [29/7]> Repeat2 Fold2 <tibble [50 × 6]> <tibble [0 × 3]>\n 8 <split [29/7]> Repeat2 Fold3 <tibble [50 × 6]> <tibble [0 × 3]>\n 9 <split [29/7]> Repeat2 Fold4 <tibble [50 × 6]> <tibble [0 × 3]>\n10 <split [29/7]> Repeat2 Fold5 <tibble [50 × 6]> <tibble [0 × 3]>\n# ℹ 15 more rows\n\n#LASSO\nglm_res <-glm_wf%>%\n  tune_grid(\n    resamples=CV5,\n    grid=glm_grid,\n    control=control_grid(save_pred = TRUE),\n    metrics=metric_set(rmse)\n  )\nglm_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times \n# A tibble: 25 × 6\n   splits         id      id2   .metrics          .notes           .predictions\n   <list>         <chr>   <chr> <list>            <list>           <list>      \n 1 <split [28/8]> Repeat1 Fold1 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 2 <split [29/7]> Repeat1 Fold2 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 3 <split [29/7]> Repeat1 Fold3 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 4 <split [29/7]> Repeat1 Fold4 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 5 <split [29/7]> Repeat1 Fold5 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 6 <split [28/8]> Repeat2 Fold1 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 7 <split [29/7]> Repeat2 Fold2 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 8 <split [29/7]> Repeat2 Fold3 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n 9 <split [29/7]> Repeat2 Fold4 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n10 <split [29/7]> Repeat2 Fold5 <tibble [30 × 5]> <tibble [0 × 3]> <tibble>    \n# ℹ 15 more rows\n\n#Random forest\n\nrf_res <- rf_wf %>%\n  tune_grid(\n    resamples=CV5,\n    grid=25,\n    control=control_grid(save_pred=TRUE),\n    metrics = metric_set(rmse))\nrf_res\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times \n# A tibble: 25 × 6\n   splits         id      id2   .metrics          .notes           .predictions\n   <list>         <chr>   <chr> <list>            <list>           <list>      \n 1 <split [28/8]> Repeat1 Fold1 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 2 <split [29/7]> Repeat1 Fold2 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 3 <split [29/7]> Repeat1 Fold3 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 4 <split [29/7]> Repeat1 Fold4 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 5 <split [29/7]> Repeat1 Fold5 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 6 <split [28/8]> Repeat2 Fold1 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 7 <split [29/7]> Repeat2 Fold2 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 8 <split [29/7]> Repeat2 Fold3 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n 9 <split [29/7]> Repeat2 Fold4 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n10 <split [29/7]> Repeat2 Fold5 <tibble [24 × 6]> <tibble [7 × 3]> <tibble>    \n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x5: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x20: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x5: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x20: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x10: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x40: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x5: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x20: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x5: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x20: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x5: 30 samples were requested but there were 28 rows in the data. 28 ...   - Warning(s) x20: 30 samples were requested but there were 28 rows in the data. 28 ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nThere are the evaluation for tree,LASSO and random forest models.\n\n#Decision tree\n#plot the result\ntree_res %>% autoplot()\n\n\n\n#choose best tree\nbest_tree <- tree_res %>%\n  select_best(\"rmse\")\n#finalizing the model\nfinal_wf <- \n  tree_wf %>% \n  finalize_workflow(best_tree)\nfinal_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 4\n\nComputational engine: rpart \n\n#fit by train data\nfinal_fit <- \n  final_wf %>%\n  fit(train_data) \n#predict value\ntree_predict <- final_fit %>%\n  predict(train_data)\n#actual vs predict\ntree_plot <- as.data.frame(cbind(train_data$`n_eggs_table eggs_all`,tree_predict$`.pred`))\nnames(tree_plot)[1:2] <- c('actual','predict')\n\ntree_plot1 <- pivot_longer(tree_plot,colnames(tree_plot)) \nplot(tree_plot,'actual','predict')\n\n\n\nboxplot(tree_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_tree <- train_data$`n_eggs_table eggs_all`-tree_predict\nplot(tree_predict$.pred,residual_tree$.pred)\n\n\n\n\n\n#LASSO\n# plot the result\nglm_res%>%\n  autoplot()\n\n\n\n# Show the best\nglm_res%>%\n  show_best(\"rmse\")\n\n# A tibble: 5 × 7\n   penalty .metric .estimator      mean     n  std_err .config              \n     <dbl> <chr>   <chr>          <dbl> <int>    <dbl> <chr>                \n1 0.0001   rmse    standard   90859754.    25 4062207. Preprocessor1_Model01\n2 0.000127 rmse    standard   90859754.    25 4062207. Preprocessor1_Model02\n3 0.000161 rmse    standard   90859754.    25 4062207. Preprocessor1_Model03\n4 0.000204 rmse    standard   90859754.    25 4062207. Preprocessor1_Model04\n5 0.000259 rmse    standard   90859754.    25 4062207. Preprocessor1_Model05\n\n# Choose the best\nglm_best <- \n  glm_res %>% \n  select_best(metric = \"rmse\")\nglm_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    <dbl> <chr>                \n1  0.0001 Preprocessor1_Model01\n\n# Finalize the workflow\nglm_final <- \n  glm_wf%>%\n  finalize_workflow(glm_best)\n#fit by train data\nglm_fit <- \n  glm_final %>%\n  fit(train_data) \n#predict value\nglm_predict <- glm_fit%>%\n  predict(train_data)\n\n#actual vs predict\nglm_plot <- as.data.frame(cbind(train_data$`n_eggs_table eggs_all`,glm_predict$`.pred`))\nnames(glm_plot)[1:2] <- c('actual','predict')\n\nglm_plot1 <- pivot_longer(glm_plot,colnames(glm_plot)) \nggboxplot(glm_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(glm_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_glm <- train_data$`n_eggs_table eggs_all`-glm_predict\nplot(glm_predict$.pred,residual_glm$.pred)\n\n\n\n#Here residual shows funnel shape. If we decide use LASSO as our final model, we should take the logarithm of our outcome.\n#Or we can try getting the lambda in Box-Cox transformation.\n\n\n#random forest\n# plot the result\nrf_res%>%\n  autoplot()\n\n\n\n# Show the best\nrf_res%>%\n  show_best(\"rmse\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator       mean     n   std_err .config             \n  <int> <int> <chr>   <chr>           <dbl> <int>     <dbl> <chr>               \n1     5     3 rmse    standard   180013240.    25  9988430. Preprocessor1_Model…\n2     3     5 rmse    standard   192819954.    25 10514856. Preprocessor1_Model…\n3     2     4 rmse    standard   200730184.    25 10486986. Preprocessor1_Model…\n4     4    15 rmse    standard   202794719.    25 11041497. Preprocessor1_Model…\n5     5    16 rmse    standard   203020236.    25 10860922. Preprocessor1_Model…\n\n# Choose the best\nrf_best <- \n  rf_res %>% \n  select_best(metric = \"rmse\")\nrf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     5     3 Preprocessor1_Model16\n\n# Finalize the workflow\nrf_final <- \n  rf_wf%>%\n  finalize_workflow(rf_best)\n#fit by train data\nrf_fit <- \n  rf_final %>%\n  fit(train_data) \n#predict value\nrf_predict <- rf_fit%>%\n  predict(train_data)\n\n#actual vs predict\nrf_plot <- as.data.frame(cbind(train_data$`n_eggs_table eggs_all`,rf_predict$`.pred`))\nnames(rf_plot)[1:2] <- c('actual','predict')\n\nrf_plot1 <- pivot_longer(rf_plot,colnames(rf_plot)) \nggboxplot(rf_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(rf_plot,'actual','predict')\n\n\n\n#residual plot\nresidual_rf <- train_data$`n_eggs_table eggs_all`-rf_predict\nplot(rf_predict$.pred,residual_rf$.pred)\n\n\n\n#residual looks fine here.\n\nHere are the model comparison\n\n# compare model performance\ntree_rmse <- tree_res%>%\n  show_best(\"rmse\")%>%\n  select(3:8)\n\nglm_rmse <- glm_res%>%\n  show_best(\"rmse\")%>%\n  select(2:7)\n\nrf_rmse <- rf_res%>%\n  show_best(\"rmse\")%>%\n  select(3:8)\n\nnull_rmse <- null_model_train %>% \n  show_best(\"rmse\")\n\nmodel <- c('null','linear','tree','lasso','random forest')\n\ncomparision <- rbind(null_rmse[1,],ln_rmse[1,],tree_rmse[1,],glm_rmse[1,],rf_rmse[1,])\n\ncomparision1 <- cbind(comparision,model)\n\n#comparision1\n\n#random forest have lowest rmse.\nn <- nrow(train_data)\nupper_chisq <- qchisq(0.025, n, lower.tail=FALSE)\nlower_chisq <- qchisq(0.025, n, lower.tail=TRUE)\n\n#95CI for each model\n\ncomparision1$lower <- comparision1$mean*sqrt(n/upper_chisq)\ncomparision1$upper <- comparision1$mean*sqrt(n/lower_chisq)\ncomparision1$`95%CI` <- paste('(',comparision1$lower,',',comparision1$upper,')')\ncomparision1\n\n  .metric .estimator      mean  n  std_err               .config         model\n1    rmse   standard 330199026 25 14382660  Preprocessor1_Model1          null\n2    rmse   standard  90989668 25  5363973  Preprocessor1_Model1        linear\n3    rmse   standard 273789798 25  9202119 Preprocessor1_Model06          tree\n4    rmse   standard  90859754 25  4062207 Preprocessor1_Model01         lasso\n5    rmse   standard 180013240 25  9988430 Preprocessor1_Model16 random forest\n      lower     upper                                   95%CI\n1 268521321 428915502 ( 268521321.395797 , 428915501.820387 )\n2  73993755 118191987  ( 73993755.1642575 , 118191987.37322 )\n3 222648743 355642140 ( 222648743.398672 , 355642140.475535 )\n4  73888108 118023234 ( 73888107.8117103 , 118023234.340879 )\n5 146388661 233830094 ( 146388660.502125 , 233830093.840236 )\n\n#so, 95% CI of random forest for random forest looks the best.\n\nWe can find LASSO is the best. Lasso is silght better than linear base on rmse.\n\n#final evaluation\n\nrf_last_fit <- rf_final%>%\n  last_fit(data_split)\nrf_last_fit%>%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator     .estimate .config             \n  <chr>   <chr>              <dbl> <chr>               \n1 rmse    standard   222237176.    Preprocessor1_Model1\n2 rsq     standard           0.764 Preprocessor1_Model1\n\nrf_predict_final <- rf_last_fit%>%\n  collect_predictions()\nrf_last_fit%>%\n  extract_fit_engine()%>%\n  vip()\n\n\n\n#performance RMSE=228677512 RSQ=0.734\n\n#actual vs predict\n\nnames(rf_predict_final)[c(4,2)] <- c('actual','predict')\n\nrf_final_plot1 <- pivot_longer(rf_predict_final,c('actual','predict')) \nggboxplot(rf_final_plot1,x='name',y='value',add = \"jitter\")\n\n\n\nplot(as.numeric(rf_predict_final$actual),as.numeric(rf_predict_final$predict))\n\n\n\n#residual plot\nresidual_rf_final <- rf_predict_final$actual-rf_predict_final$predict\nplot(rf_predict_final$predict,residual_rf_final)\n\n\n\n\nWe can see that predict vs actual plot is OK. Residual plot is OK. We can also find the predictor importance."
  },
  {
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "Placeholder file for the future visualization exercise.\nsource https://fivethirtyeight.com/features/how-every-nfl-teams-fans-lean-politically/\ndata https://github.com/fivethirtyeight/data/tree/master/nfl-fandom\n\nlibrary(naniar)\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(cowplot)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(ggplot2)\n\ndata1 <- read.csv(\"mod5/NFL_fandom_data-google_trends.csv\",header = TRUE,skip=1)\n\na <- miss_var_summary(data1)\nglimpse(data1)\n\nRows: 207\nColumns: 9\n$ DMA              <chr> \"Abilene-Sweetwater TX\", \"Albany GA\", \"Albany-Schenec…\n$ NFL              <chr> \"45%\", \"32%\", \"40%\", \"53%\", \"42%\", \"28%\", \"47%\", \"36%…\n$ NBA              <chr> \"21%\", \"30%\", \"20%\", \"21%\", \"28%\", \"13%\", \"22%\", \"28%…\n$ MLB              <chr> \"14%\", \"9%\", \"20%\", \"11%\", \"9%\", \"21%\", \"12%\", \"10%\",…\n$ NHL              <chr> \"2%\", \"1%\", \"8%\", \"3%\", \"1%\", \"12%\", \"2%\", \"3%\", \"3%\"…\n$ NASCAR           <chr> \"4%\", \"8%\", \"6%\", \"3%\", \"5%\", \"10%\", \"5%\", \"5%\", \"8%\"…\n$ CBB              <chr> \"3%\", \"3%\", \"3%\", \"4%\", \"3%\", \"7%\", \"3%\", \"5%\", \"4%\",…\n$ CFB              <chr> \"11%\", \"17%\", \"4%\", \"6%\", \"12%\", \"9%\", \"10%\", \"14%\", …\n$ Trump.2016.Vote. <chr> \"79.13%\", \"59.12%\", \"44.11%\", \"39.58%\", \"69.64%\", \"63…\n\ndata2 <-data1\n\ndata2[,1] <- data1[,1]\ndata2[,2:9] <- data.frame(sapply(data1[,2:9], function(x) as.numeric(gsub(\"%\", \"\", x))))\n\ncolnames(data2)[9] =\"Trumpvote\"\n\nglimpse(data2)\n\nRows: 207\nColumns: 9\n$ DMA       <chr> \"Abilene-Sweetwater TX\", \"Albany GA\", \"Albany-Schenectady-Tr…\n$ NFL       <dbl> 45, 32, 40, 53, 42, 28, 47, 36, 39, 40, 48, 45, 39, 40, 45, …\n$ NBA       <dbl> 21, 30, 20, 21, 28, 13, 22, 28, 26, 29, 26, 20, 20, 29, 27, …\n$ MLB       <dbl> 14, 9, 20, 11, 9, 21, 12, 10, 8, 11, 14, 20, 19, 8, 12, 13, …\n$ NHL       <dbl> 2, 1, 8, 3, 1, 12, 2, 3, 3, 3, 3, 5, 9, 2, 1, 4, 5, 2, 9, 2,…\n$ NASCAR    <dbl> 4, 8, 6, 3, 5, 10, 5, 5, 8, 3, 4, 3, 8, 3, 4, 4, 5, 6, 7, 6,…\n$ CBB       <dbl> 3, 3, 3, 4, 3, 7, 3, 5, 4, 3, 1, 3, 2, 3, 2, 3, 4, 3, 3, 6, …\n$ CFB       <dbl> 11, 17, 4, 6, 12, 9, 10, 14, 13, 11, 3, 4, 3, 15, 9, 13, 7, …\n$ Trumpvote <dbl> 79.13, 59.12, 44.11, 39.58, 69.64, 63.61, 77.54, 47.58, 52.7…\n\np1 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = NBA))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"NBA\")+geom_smooth(method = \"lm\")\n\n\n\np2 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = MLB))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"MLB\")+geom_smooth(method = \"lm\")\n\np3 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = NHL))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"NHL\")+geom_smooth(method = \"lm\")\n\np4 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = NFL))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"NFL\")+geom_smooth(method = \"lm\")\n\np5 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = CBB))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"CBB\")+geom_smooth(method = \"lm\")\n\np6 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = NASCAR))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"NASCAR\")+geom_smooth(method = \"lm\")\n\np7 <- ggplot(data = data2, mapping = aes(x = Trumpvote, y = CFB))+geom_point()+xlim(20,80)+ylim(0,50)+ggtitle(\"CFB\")+geom_smooth(method = \"lm\")\n\n\ngrid.arrange(p1,p2,p3,p4,p5,p6,p7, ncol=7, nrow =1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\nRemoved 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nFirst plot is finished. And second plot include a map which I am not familiar with now. Maybe I will learn it next time.\n\ndata3 <- read.csv(\"mod5/NFL_fandom_data-surveymonkey.csv\",header = TRUE,skip=1)\n\nInitial data is not good to plot. So we need to do data cleaning and generate a new data frame.\n\n#seems wrong,just leave it here in case I need to come back\ndata4 <- data3 %>% select(c(1,21))\n\ndata4[34:66,] <- data3 %>% select(c(1,22))\n\ndata4[67:99,] <- data3 %>% select(c(1,23))\n\ndata4[,3] <- rep(c('GOP','DEM','IND'),each=33)\n\ncolnames(data4)[2:3] =c(\"percent\",'partisan')\n\n ggplot(data4, aes(x = Team, y = percent))+\n  geom_col(aes(fill = partisan))+ coord_flip()\n\n\n\n\nThen rank by difference.\n\ndata5 <- data3 %>% select(c(1,21:23))\ncolnames(data5)[2:4] <- c('gop','dem','ind')\nglimpse(data5)\n\nRows: 33\nColumns: 4\n$ Team <chr> \"Arizona Cardinals\", \"Atlanta Falcons\", \"Baltimore Ravens\", \"Buff…\n$ gop  <chr> \"26%\", \"29%\", \"19%\", \"26%\", \"30%\", \"22%\", \"35%\", \"28%\", \"32%\", \"2…\n$ dem  <chr> \"26%\", \"31%\", \"37%\", \"24%\", \"31%\", \"33%\", \"35%\", \"32%\", \"29%\", \"3…\n$ ind  <chr> \"48%\", \"40%\", \"43%\", \"50%\", \"39%\", \"45%\", \"30%\", \"40%\", \"39%\", \"3…\n\n data5[,2:4] <- data.frame(sapply(data5[,2:4], function(x) as.numeric(gsub(\"%\", \"\", x))))\n \n glimpse(data5)\n\nRows: 33\nColumns: 4\n$ Team <chr> \"Arizona Cardinals\", \"Atlanta Falcons\", \"Baltimore Ravens\", \"Buff…\n$ gop  <dbl> 26, 29, 19, 26, 30, 22, 35, 28, 32, 29, 18, 24, 31, 25, 29, 28, 2…\n$ dem  <dbl> 26, 31, 37, 24, 31, 33, 35, 32, 29, 32, 38, 33, 26, 30, 27, 28, 3…\n$ ind  <dbl> 48, 40, 43, 50, 39, 45, 30, 40, 39, 39, 44, 44, 44, 44, 44, 44, 4…\n\n data5$dif <- data5$dem-data5$gop\ndata5 <- data5[order(-data5$dif),]\ndata5$rank <- 1:33\ndata5 <- data5[order(data5$Team),]\n\n\ndata4 <- data4[order(data4$Team),]\ndata4$rank <- rep(data5$rank,each=3)\n\ndata6 <- data4[order(data4$rank),]\n\nlevel <- c('GOP','IND','DEM')\n\n\n\n  data6[,2] <- data.frame(sapply(data6[,2], function(x) as.numeric(gsub(\"%\", \"\", x))))\n  \n   ggplot(data6, aes(x = reorder(Team,-rank), y = percent))+\n  geom_col(aes(fill = factor(partisan,levels=level)))+ coord_flip()"
  }
]